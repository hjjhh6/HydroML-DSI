{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b01e384c",
   "metadata": {},
   "source": [
    "This code has been modified from the absolute path in the original version to a relative path, so there may be path bugs. Please be aware of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15998d3c",
   "metadata": {},
   "source": [
    "Please note that the sampling data generation part of 1-1 and 1-2 only requires one execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561cce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "\n",
    "\n",
    "# Load Forest CoverType dataset\n",
    "\n",
    "\n",
    "forest_cover = fetch_openml(name=\"covertype\", version=1, as_frame=True)\n",
    "\n",
    "\n",
    "original_df = forest_cover.frame\n",
    "\n",
    "\n",
    "\n",
    "# Ensure all variables are numerical\n",
    "\n",
    "\n",
    "numerical_columns = original_df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "\n",
    "df = original_df[numerical_columns]\n",
    "\n",
    "\n",
    "\n",
    "# Remove the last four columns from the dataframe\n",
    "\n",
    "\n",
    "df = df.iloc[:, :-4]\n",
    "\n",
    "\n",
    "\n",
    "# Extract the Cover_Type column and add it to df as the class column\n",
    "\n",
    "\n",
    "class_df = df.copy()\n",
    "\n",
    "\n",
    "class_df[\"class\"] = forest_cover.frame[\"class\"]\n",
    "\n",
    "\n",
    "\n",
    "# Display the first few rows of the processed dataset\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "# Use your data and variable list to create sample combinations\n",
    "\n",
    "\n",
    "variables = df.columns.tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Categorize variables based on physical significance\n",
    "\n",
    "\n",
    "variable_groups = [\n",
    "    [\"elevation\", \"aspect\", \"slope\"],  # Terrain features\n",
    "    [\n",
    "        \"horizontal_distance_to_hydrology\",\n",
    "        \"Vertical_Distance_To_Hydrology\",\n",
    "    ],  # Hydrology features\n",
    "    [\"Horizontal_Distance_To_Roadways\"],  # Road features\n",
    "    [\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"],  # Hillshade features\n",
    "    [\"Horizontal_Distance_To_Fire_Points\"],  # Fire point features\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Ensure all variables are in the variable list\n",
    "\n",
    "\n",
    "for group in variable_groups:\n",
    "\n",
    "\n",
    "    for variable in group:\n",
    "\n",
    "\n",
    "        if variable not in variables:\n",
    "\n",
    "\n",
    "            print(f\"Warning: {variable} is not in the variables list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ec45b",
   "metadata": {},
   "source": [
    "Fuzzy c-means clustering sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aa4dee",
   "metadata": {},
   "source": [
    "Type 1: Number of clusters equals the number of class categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import skfuzzy as fuzz\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Define sample sizes and the number of samples to generate\n",
    "sample_sizes = [1000, 5000, 10000, 20000]\n",
    "num_samples_per_size = 20\n",
    "\n",
    "# Get the number of class categories\n",
    "n_clusters = class_df[\"class\"].nunique()\n",
    "\n",
    "\n",
    "# Define the function to generate sample sets\n",
    "def generate_samples(sample_size, random_state):\n",
    "    # Perform fuzzy c-means clustering\n",
    "    _, u, _, d, _, _, _ = fuzz.cluster.cmeans(\n",
    "        df_scaled.T,\n",
    "        n_clusters,\n",
    "        2,\n",
    "        error=0.005,\n",
    "        maxiter=1000,\n",
    "        init=None,\n",
    "        seed=random_state,\n",
    "    )\n",
    "\n",
    "    # Get the fuzzy membership for each data point\n",
    "    fuzzy_membership = u.T\n",
    "\n",
    "    # Calculate the number of samples for each cluster\n",
    "    cluster_counts = np.sum(fuzzy_membership, axis=0)\n",
    "\n",
    "    # Calculate the number of samples to extract from each cluster\n",
    "    sample_size_per_cluster = (cluster_counts / cluster_counts.sum()) * sample_size\n",
    "    sample_size_per_cluster = sample_size_per_cluster.astype(int)\n",
    "\n",
    "    # Ensure the total number of samples equals sample_size\n",
    "    difference = sample_size - sample_size_per_cluster.sum()\n",
    "    if difference > 0:\n",
    "        sample_size_per_cluster[np.argmin(sample_size_per_cluster)] += difference\n",
    "    elif difference < 0:\n",
    "        sample_size_per_cluster[np.argmax(sample_size_per_cluster)] += difference\n",
    "\n",
    "    # Initialize variables\n",
    "    sampled_indices = set()\n",
    "    sampled_cluster = np.full(df_scaled.shape[0], -1)  # -1 indicates not sampled\n",
    "    sample_status = np.full(\n",
    "        (df_scaled.shape[0], n_clusters), 0\n",
    "    )  # 0: not sampled, 1: sampled, 2: reclassified after sampling\n",
    "    remaining_samples_per_cluster = sample_size_per_cluster.copy()\n",
    "\n",
    "    def resample_cluster(\n",
    "        cluster_idx,\n",
    "        sampled_indices,\n",
    "        sampled_cluster,\n",
    "        sample_status,\n",
    "        remaining_samples_per_cluster,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Resample samples from the specified cluster.\n",
    "\n",
    "        Parameters:\n",
    "        - cluster_idx: Index of the current cluster\n",
    "        - sampled_indices: Set of sampled indices\n",
    "        - sampled_cluster: Cluster assignment for each sample\n",
    "        - sample_status: Sampling status for each sample in each cluster\n",
    "        - remaining_samples_per_cluster: Remaining samples to extract from each cluster\n",
    "        \"\"\"\n",
    "        # Select unsampled points\n",
    "        cluster_indices_j = np.where(sample_status[:, cluster_idx] == 0)[0]\n",
    "        # Sort by membership\n",
    "        memberships_j = fuzzy_membership[cluster_indices_j, cluster_idx]\n",
    "        # Sort indices by membership in descending order\n",
    "        sorted_indices_j = cluster_indices_j[np.argsort(-memberships_j)]\n",
    "        # Extract samples\n",
    "        for idx_j in sorted_indices_j:\n",
    "            if remaining_samples_per_cluster[cluster_idx] <= 0:\n",
    "                break\n",
    "            if sample_status[idx_j, cluster_idx] == 0:\n",
    "                if sampled_cluster[idx_j] == -1:\n",
    "                    # Sample not yet extracted, directly sample\n",
    "                    sampled_indices.add(idx_j)\n",
    "                    sampled_cluster[idx_j] = cluster_idx\n",
    "                    sample_status[idx_j, cluster_idx] = 1  # Sampled\n",
    "                    remaining_samples_per_cluster[cluster_idx] -= 1\n",
    "                    return\n",
    "                else:\n",
    "                    # Sample already extracted by another cluster, handle membership conflict\n",
    "                    original_cluster = sampled_cluster[idx_j]\n",
    "                    if (\n",
    "                        fuzzy_membership[idx_j, cluster_idx]\n",
    "                        > fuzzy_membership[idx_j, original_cluster]\n",
    "                    ):\n",
    "                        sampled_cluster[idx_j] = cluster_idx\n",
    "                        sample_status[idx_j, original_cluster] = (\n",
    "                            2  # Reclassified after sampling\n",
    "                        )\n",
    "                        sample_status[idx_j, cluster_idx] = 1  # Sampled\n",
    "                        remaining_samples_per_cluster[cluster_idx] -= 1\n",
    "                        remaining_samples_per_cluster[original_cluster] += 1\n",
    "                        # Resample the cluster from which the sample was removed\n",
    "                        resample_cluster(\n",
    "                            original_cluster,\n",
    "                            sampled_indices,\n",
    "                            sampled_cluster,\n",
    "                            sample_status,\n",
    "                            remaining_samples_per_cluster,\n",
    "                        )\n",
    "                        return\n",
    "\n",
    "    # Extract samples based on relative distance to centroids and handle membership conflicts\n",
    "    for i in range(n_clusters):\n",
    "        cluster_indices = np.arange(df_scaled.shape[0])  # Select all points\n",
    "        if len(cluster_indices) > 0:\n",
    "            # Use distances from the d matrix\n",
    "            distances = d[i, cluster_indices]\n",
    "            # Sort indices by distance in ascending order\n",
    "            sorted_indices = cluster_indices[np.argsort(distances)]\n",
    "            # Extract samples\n",
    "            for idx in sorted_indices:\n",
    "                if remaining_samples_per_cluster[i] <= 0:\n",
    "                    break\n",
    "                if sampled_cluster[idx] == -1:\n",
    "                    # Sample not yet extracted, directly sample\n",
    "                    sampled_indices.add(idx)\n",
    "                    sampled_cluster[idx] = i\n",
    "                    sample_status[idx] = 1  # Sampled\n",
    "                    remaining_samples_per_cluster[i] -= 1\n",
    "                else:\n",
    "                    # Sample already extracted by another cluster, handle membership conflict\n",
    "                    current_cluster = sampled_cluster[idx]\n",
    "                    if (\n",
    "                        fuzzy_membership[idx, i]\n",
    "                        > fuzzy_membership[idx, current_cluster]\n",
    "                    ):\n",
    "                        sampled_cluster[idx] = i\n",
    "                        sample_status[idx, current_cluster] = (\n",
    "                            2  # Reclassified after sampling\n",
    "                        )\n",
    "                        sample_status[idx, i] = 1  # Sampled\n",
    "                        remaining_samples_per_cluster[i] -= 1\n",
    "                        remaining_samples_per_cluster[current_cluster] += 1\n",
    "                        # Resample the cluster from which the sample was removed\n",
    "                        resample_cluster(\n",
    "                            current_cluster,\n",
    "                            sampled_indices,\n",
    "                            sampled_cluster,\n",
    "                            sample_status,\n",
    "                            remaining_samples_per_cluster,\n",
    "                        )\n",
    "\n",
    "    # Get the sampled data\n",
    "    sampled_df = df.loc[list(sampled_indices)].copy()\n",
    "    # Add the class column\n",
    "    sampled_df[\"class\"] = class_df.loc[sampled_df.index, \"class\"]\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "# Specify the save path\n",
    "save_path = \"sampleddata/combined_samples/\"\n",
    "\n",
    "# Create the save path directory (if it doesn't exist)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define the function to generate sample sets in parallel\n",
    "def parallel_generate_samples(sample_size, i):\n",
    "    random_state = np.random.randint(0, 10000)  # Generate random seed\n",
    "    sampled_df = generate_samples(sample_size, random_state)\n",
    "    sampled_df.to_csv(\n",
    "        os.path.join(save_path, f\"FCMtp1sampled_data_{sample_size}_set_{i+1}.csv\")\n",
    "    )\n",
    "    print(\n",
    "        f\"Generated sample set {i+1} for sample size {sample_size} with random state {random_state}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Generate multiple sample sets in parallel\n",
    "for sample_size in sample_sizes:\n",
    "    Parallel(n_jobs=10, verbose=10)(\n",
    "        delayed(parallel_generate_samples)(sample_size, i)\n",
    "        for i in range(num_samples_per_size)\n",
    "    )\n",
    "\n",
    "print(\"All sample sets generated and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc396df4",
   "metadata": {},
   "source": [
    "Type 2: Number of clusters equals the number of sampling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample sizes and the number of samples to generate\n",
    "sample_sizes = [1000, 5000, 10000, 20000]\n",
    "num_samples_per_size = 20\n",
    "\n",
    "# Get the number of class categories\n",
    "n_clusters = class_df[\"class\"].nunique()\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Define sample sizes and the number of samples to generate\n",
    "sample_sizes = [1000, 5000, 10000, 20000]\n",
    "num_samples_per_size = 20\n",
    "\n",
    "\n",
    "# Define the function to generate sample sets\n",
    "def generate_samples(sample_size, random_state):\n",
    "    n_clusters = sample_size  # Set the number of clusters equal to the sample size\n",
    "\n",
    "    # Perform fuzzy c-means clustering\n",
    "    _, u, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
    "        df_scaled.T,\n",
    "        n_clusters,\n",
    "        2,\n",
    "        error=0.005,\n",
    "        maxiter=1000,\n",
    "        init=None,\n",
    "        seed=random_state,\n",
    "    )\n",
    "\n",
    "    # Initialize variables\n",
    "    sampled_indices = set()\n",
    "    sampled_cluster = np.full(\n",
    "        df_scaled.shape[0], -1\n",
    "    )  # -1 indicates not sampled, others indicate the corresponding cluster\n",
    "    sample_status = np.full(\n",
    "        (df_scaled.shape[0], n_clusters), 0\n",
    "    )  # 0: not sampled, 1: sampled, 2: reclassified after sampling\n",
    "    remaining_samples_per_cluster = np.ones(\n",
    "        n_clusters, dtype=int\n",
    "    )  # Each cluster extracts only 1 sample\n",
    "\n",
    "    def resample_cluster(\n",
    "        cluster_idx,\n",
    "        sampled_indices,\n",
    "        sampled_cluster,\n",
    "        sample_status,\n",
    "        remaining_samples_per_cluster,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Resample samples from the specified cluster.\n",
    "\n",
    "        Parameters:\n",
    "        - cluster_idx: Index of the current cluster\n",
    "        - sampled_indices: Set of sampled indices\n",
    "        - sampled_cluster: Cluster assignment for each sample\n",
    "        - sample_status: Sampling status for each sample in each cluster\n",
    "        - remaining_samples_per_cluster: Remaining samples to extract from each cluster\n",
    "        \"\"\"\n",
    "        # Select unsampled points\n",
    "        cluster_indices_j = np.where(sample_status[:, cluster_idx] == 0)[0]\n",
    "        # Sort by membership\n",
    "        memberships_j = u[cluster_idx, cluster_indices_j]\n",
    "        # Sort indices by membership in descending order\n",
    "        sorted_indices_j = cluster_indices_j[np.argsort(-memberships_j)]\n",
    "        # Extract samples\n",
    "        for idx_j in sorted_indices_j:\n",
    "            if remaining_samples_per_cluster[cluster_idx] <= 0:\n",
    "                break\n",
    "            if sample_status[idx_j, cluster_idx] == 0:\n",
    "                if sampled_cluster[idx_j] == -1:\n",
    "                    # Sample not yet extracted, directly sample\n",
    "                    sampled_indices.add(idx_j)\n",
    "                    sampled_cluster[idx_j] = cluster_idx\n",
    "                    sample_status[idx_j, cluster_idx] = 1  # Sampled\n",
    "                    remaining_samples_per_cluster[cluster_idx] -= 1\n",
    "                    return\n",
    "                else:\n",
    "                    # Sample already extracted by another cluster, handle membership conflict\n",
    "                    original_cluster = sampled_cluster[idx_j]\n",
    "                    if u[cluster_idx, idx_j] > u[original_cluster, idx_j]:\n",
    "                        sampled_cluster[idx_j] = cluster_idx\n",
    "                        sample_status[idx_j, original_cluster] = (\n",
    "                            2  # Reclassified after sampling\n",
    "                        )\n",
    "                        sample_status[idx_j, cluster_idx] = 1  # Sampled\n",
    "                        remaining_samples_per_cluster[cluster_idx] -= 1\n",
    "                        remaining_samples_per_cluster[original_cluster] += 1\n",
    "                        # Resample the cluster from which the sample was removed\n",
    "                        resample_cluster(\n",
    "                            original_cluster,\n",
    "                            sampled_indices,\n",
    "                            sampled_cluster,\n",
    "                            sample_status,\n",
    "                            remaining_samples_per_cluster,\n",
    "                        )\n",
    "                        return\n",
    "\n",
    "    # Extract samples based on membership and handle membership conflicts\n",
    "    for i in range(n_clusters):\n",
    "        cluster_indices = np.arange(df_scaled.shape[0])  # Select all points\n",
    "        if len(cluster_indices) > 0:\n",
    "            # Sort by membership\n",
    "            memberships = u[i, cluster_indices]\n",
    "            # Sort indices by membership in descending order\n",
    "            sorted_indices = cluster_indices[np.argsort(-memberships)]\n",
    "            # Extract samples\n",
    "            for idx in sorted_indices:\n",
    "                if remaining_samples_per_cluster[i] <= 0:\n",
    "                    break\n",
    "                if sampled_cluster[idx] == -1:\n",
    "                    sampled_indices.add(idx)\n",
    "                    sampled_cluster[idx] = i\n",
    "                    sample_status[idx, i] = 1  # Sampled\n",
    "                    remaining_samples_per_cluster[i] -= 1\n",
    "                else:\n",
    "                    # Sample already extracted by another cluster, handle membership conflict\n",
    "                    current_cluster = sampled_cluster[idx]\n",
    "                    if u[i, idx] > u[current_cluster, idx]:\n",
    "                        sampled_cluster[idx] = i\n",
    "                        sample_status[idx, current_cluster] = (\n",
    "                            2  # Reclassified after sampling\n",
    "                        )\n",
    "                        sample_status[idx, i] = 1  # Sampled\n",
    "                        remaining_samples_per_cluster[i] -= 1\n",
    "                        remaining_samples_per_cluster[current_cluster] += 1\n",
    "                        # Resample the cluster from which the sample was removed\n",
    "                        resample_cluster(\n",
    "                            current_cluster,\n",
    "                            sampled_indices,\n",
    "                            sampled_cluster,\n",
    "                            sample_status,\n",
    "                            remaining_samples_per_cluster,\n",
    "                        )\n",
    "\n",
    "    # Get the sampled data\n",
    "    sampled_df = df.loc[list(sampled_indices)].copy()\n",
    "    # Add the class column\n",
    "    sampled_df[\"class\"] = class_df.loc[sampled_df.index, \"class\"]\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "# Specify the save path\n",
    "save_path = \"sampleddata/combined_samples/\"\n",
    "\n",
    "# Create the save path directory (if it doesn't exist)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Generate multiple sample sets\n",
    "for sample_size in sample_sizes:\n",
    "    for i in range(num_samples_per_size):\n",
    "        random_state = np.random.randint(0, 10000)  # Generate random seed\n",
    "        sampled_df = generate_samples(sample_size, random_state)\n",
    "        sampled_df.to_csv(\n",
    "            os.path.join(save_path, f\"FCMtp2sampled_data_{sample_size}_set_{i+1}.csv\")\n",
    "        )\n",
    "        print(\n",
    "            f\"Generated sample set {i+1} for sample size {sample_size} with random state {random_state}\"\n",
    "        )\n",
    "\n",
    "print(\"All sample sets generated and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b43d6",
   "metadata": {},
   "source": [
    "CLHS Conditioned Latin hypercube sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b078d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clhs import clhs\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Define sample sizes and the number of samples to generate\n",
    "sample_sizes = [1000, 5000, 10000, 20000]\n",
    "num_samples_per_size = 20\n",
    "\n",
    "# Specify the save path\n",
    "save_path = \"sampleddata/combined_samples/\"\n",
    "\n",
    "# Create the save path directory (if it doesn't exist)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define the parallel processing function\n",
    "def parallel_clhs(sample_size, i, seed):\n",
    "    np.random.seed(seed)\n",
    "    clhs_sampled_results = clhs(df_scaled, sample_size)\n",
    "    clhs_sampled_df = class_df.iloc[clhs_sampled_results[\"sample_indices\"]]\n",
    "    file_path = os.path.join(\n",
    "        save_path, f\"clhs_sampled_data_{sample_size}_set_{i+1}.csv\"\n",
    "    )\n",
    "    clhs_sampled_df.to_csv(file_path)\n",
    "    print(f\"Generated sample set {i+1} for sample size {sample_size}\")\n",
    "\n",
    "\n",
    "# Perform sampling in parallel using joblib\n",
    "Parallel(n_jobs=10, verbose=30)(\n",
    "    delayed(parallel_clhs)(size, i, np.random.randint(0, 10000))\n",
    "    for size in sample_sizes\n",
    "    for i in range(num_samples_per_size)\n",
    ")\n",
    "\n",
    "print(\"All sample sets generated and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316774b6",
   "metadata": {},
   "source": [
    "k-means sampling, number of clusters equals the number of class categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ce689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "\n",
    "\n",
    "# Define sample sizes and the number of samples to generate\n",
    "\n",
    "\n",
    "sample_sizes = [1000, 5000, 10000, 20000]\n",
    "\n",
    "\n",
    "num_samples_per_size = 20\n",
    "\n",
    "\n",
    "\n",
    "# Get the number of class categories\n",
    "\n",
    "\n",
    "n_clusters = class_df[\"class\"].nunique()\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to generate sample sets\n",
    "\n",
    "\n",
    "\n",
    "def generate_kmeans_samples(sample_size, random_state):\n",
    "\n",
    "\n",
    "    # Perform clustering using KMeans\n",
    "\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "\n",
    "\n",
    "    kmeans.fit(df_scaled)\n",
    "\n",
    "\n",
    "    # Get the labels for each cluster\n",
    "\n",
    "\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "\n",
    "    # Calculate the number of samples for each cluster\n",
    "\n",
    "\n",
    "    cluster_counts = np.bincount(labels, minlength=n_clusters)\n",
    "\n",
    "\n",
    "    # Calculate the number of samples to extract from each cluster\n",
    "\n",
    "\n",
    "    sample_size_per_cluster = (cluster_counts / cluster_counts.sum()) * sample_size\n",
    "\n",
    "\n",
    "    sample_size_per_cluster = sample_size_per_cluster.astype(int)\n",
    "\n",
    "\n",
    "    # Ensure the total number of samples equals sample_size\n",
    "\n",
    "\n",
    "    difference = sample_size - sample_size_per_cluster.sum()\n",
    "\n",
    "\n",
    "    if difference > 0:\n",
    "\n",
    "\n",
    "        sample_size_per_cluster[np.argmin(sample_size_per_cluster)] += difference\n",
    "\n",
    "\n",
    "    elif difference < 0:\n",
    "\n",
    "\n",
    "        sample_size_per_cluster[np.argmax(sample_size_per_cluster)] += difference\n",
    "\n",
    "\n",
    "    # Initialize variables\n",
    "\n",
    "\n",
    "    sampled_indices = []\n",
    "\n",
    "\n",
    "    # Extract samples based on relative distance to centroids\n",
    "\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "\n",
    "\n",
    "        cluster_indices = np.where(labels == i)[0]\n",
    "\n",
    "\n",
    "        if len(cluster_indices) > 0:\n",
    "\n",
    "\n",
    "            # Use distances from the distance matrix\n",
    "\n",
    "\n",
    "            distances = np.linalg.norm(\n",
    "                df_scaled[cluster_indices] - kmeans.cluster_centers_[i], axis=1\n",
    "            )\n",
    "\n",
    "\n",
    "            # Sort indices by distance in ascending order\n",
    "\n",
    "\n",
    "            sorted_indices = cluster_indices[np.argsort(distances)]\n",
    "\n",
    "\n",
    "            # Extract samples\n",
    "\n",
    "\n",
    "            sampled_indices.extend(sorted_indices[: sample_size_per_cluster[i]])\n",
    "\n",
    "\n",
    "    # Get the sampled data\n",
    "\n",
    "\n",
    "    sampled_df = df.loc[sampled_indices].copy()\n",
    "\n",
    "\n",
    "    # Add the class column\n",
    "\n",
    "\n",
    "    sampled_df[\"class\"] = class_df.loc[sampled_df.index, \"class\"]\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "\n",
    "# Specify the save path\n",
    "\n",
    "\n",
    "save_path = \"sampleddata/combined_samples/\"\n",
    "\n",
    "\n",
    "\n",
    "# Create the save path directory (if it doesn't exist)\n",
    "\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define the parallel function to generate sample sets\n",
    "\n",
    "\n",
    "\n",
    "def parallel_generate_kmeans_samples(sample_size, i):\n",
    "\n",
    "\n",
    "    random_state = np.random.randint(0, 10000)  # Generate random seed\n",
    "\n",
    "\n",
    "    sampled_df = generate_kmeans_samples(sample_size, random_state)\n",
    "\n",
    "\n",
    "    file_path = os.path.join(\n",
    "        save_path, f\"kmeans_sampled_data_{sample_size}_set_{i+1}.csv\"\n",
    "    )\n",
    "\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "\n",
    "\n",
    "        print(f\"File {file_path} already exists, skipping...\")\n",
    "        return\n",
    "\n",
    "\n",
    "    sampled_df.to_csv(file_path)\n",
    "    print(\n",
    "\n",
    "        f\"Generated sample set {i+1} for sample size {sample_size} with random state {random_state}\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Perform sampling in parallel using joblib\n",
    "\n",
    "\n",
    "Parallel(n_jobs=10, verbose=30)(\n",
    "    delayed(parallel_generate_kmeans_samples)(size, i)\n",
    "    for size in sample_sizes\n",
    "    for i in range(num_samples_per_size)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"All sample sets generated and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae03a9d",
   "metadata": {},
   "source": [
    "From the cell below, it can be run independently. This is the model construction section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Read the original dataset\n",
    "class_df = pd.read_csv(\"sampleddata/class_df.csv\")\n",
    "\n",
    "# Define sample levels and sampling methods\n",
    "sample_levels = [1000, 5000, 10000, 20000]\n",
    "\n",
    "sampling_methods = [\n",
    "    \"BalancedSampling_sampled_data\",\n",
    "    \"clhs_sampled_data\",\n",
    "    \"FCMtp1sampled_data\",\n",
    "    \"FCMtp2sampled_data\",\n",
    "    \"FSCS_sampled_data\",\n",
    "    \"kmeans_sampled_data\",\n",
    "]\n",
    "\n",
    "# Define covariates and target variable\n",
    "covariates = [\n",
    "    \"elevation\",\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"horizontal_distance_to_hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\",\n",
    "    \"Hillshade_Noon\",\n",
    "    \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\",\n",
    "]\n",
    "target = \"class\"\n",
    "\n",
    "# Store training and testing sets\n",
    "train_test_data = {}\n",
    "\n",
    "# Iterate through each sample level and sampling method\n",
    "for level in sample_levels:\n",
    "    for t in range(1, 21):\n",
    "        for method in sampling_methods:\n",
    "            sample_file = f\"sampleddata/combined_samples/{method}_{level}_set_{t}.csv\"\n",
    "\n",
    "            if not os.path.exists(sample_file):\n",
    "                print(f\"File {sample_file} does not exist. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            sample_data = pd.read_csv(sample_file, index_col=0)\n",
    "            X_train = sample_data[covariates]\n",
    "            y_train = sample_data[target]\n",
    "            train_indices = sample_data.index\n",
    "\n",
    "            # Generate the corresponding test set\n",
    "            test_data = class_df.drop(train_indices)\n",
    "            X_test = test_data[covariates]\n",
    "            y_test = test_data[target]\n",
    "\n",
    "            # Store training and testing sets\n",
    "            train_test_data[f\"{method}_{level}_set_{t}\"] = (\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_test,\n",
    "                y_test,\n",
    "            )\n",
    "\n",
    "# Generate SRS sample subsets\n",
    "for level in sample_levels:\n",
    "    for t in range(1, 21):\n",
    "        # Perform simple random sampling\n",
    "        srs_sample = resample(class_df, n_samples=level, random_state=42 + t)\n",
    "        X_train = srs_sample[covariates]\n",
    "        y_train = srs_sample[target]\n",
    "        train_indices = srs_sample.index\n",
    "\n",
    "        # Generate the corresponding test set\n",
    "        test_data = class_df.drop(train_indices)\n",
    "        X_test = test_data[covariates]\n",
    "        y_test = test_data[target]\n",
    "\n",
    "        # Store training and testing sets\n",
    "        train_test_data[f\"SRS_sampled_df_{level}_set_{t}\"] = (\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "        )\n",
    "\n",
    "print(\"All training and testing sets generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a62f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear unnecessary variables to free up memory.\n",
    "del (\n",
    "    class_df,\n",
    "    sample_data,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    train_indices,\n",
    "    test_data,\n",
    "    srs_sample,\n",
    ")\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f04aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "import lightgbm as lgb\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna hyperparameter optimization\n",
    "def objective(trial, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "\n",
    "    Parameters:\n",
    "    trial: Optuna Trial object for recording trial results.\n",
    "    X_train: Training feature data.\n",
    "    y_train: Training label data.\n",
    "\n",
    "    Returns:\n",
    "    Mean multi-class log loss from cross-validation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use LabelEncoder to convert y_train to integer variables\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "\n",
    "    # Define the LightGBM parameter search space\n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",  # Multi-class classification task\n",
    "        \"metric\": \"multi_logloss\",  # Multi-class log loss\n",
    "        \"boosting_type\": \"gbdt\",  # Gradient boosting decision tree\n",
    "        \"seed\": 42,  # Random seed\n",
    "        \"num_class\": len(np.unique(y_train)),  # Number of classes\n",
    "        \"device\": \"cpu\",  # Use CPU for training\n",
    "        \"verbosity\": -1,  # Set to -1 to reduce output\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 512),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.1, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.1, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 0, 15),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 100),\n",
    "    }\n",
    "\n",
    "    # Create the LightGBM dataset\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    # Perform cross-validation with LightGBM\n",
    "    cv_results = lgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        nfold=10,\n",
    "        seed=42,\n",
    "        num_boost_round=100,\n",
    "    )\n",
    "\n",
    "    mean_logloss = cv_results[\"valid multi_logloss-mean\"][-1]\n",
    "    # Store the best num_boost_round in user_attrs\n",
    "    trial.set_user_attr(\"num_boost_round\", len(cv_results[\"valid multi_logloss-mean\"]))\n",
    "\n",
    "    return mean_logloss\n",
    "\n",
    "\n",
    "# Set Optuna log level to WARNING to reduce output\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "\n",
    "# Define the parallel processing function\n",
    "def optimize_and_evaluate(key):\n",
    "    # Create an Optuna study object\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    # Select a dataset for optimization and evaluation\n",
    "    X_train, y_train, X_test, y_test = train_test_data[key]\n",
    "    print(f\"Processing dataset: {key}\")\n",
    "\n",
    "    # Run hyperparameter optimization, set up to 100 trials\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train, y_train, X_test, y_test),\n",
    "        n_trials=100,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Add necessary parameters\n",
    "    best_params.update(\n",
    "        {\n",
    "            \"objective\": \"multiclass\",\n",
    "            \"metric\": \"multi_logloss\",\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"seed\": 42,\n",
    "            \"num_class\": len(np.unique(y_train)),\n",
    "            \"device\": \"cpu\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Use LabelEncoder to convert y_train and y_test to integer variables\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "    # Train the model with the best hyperparameters\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dtest = lgb.Dataset(X_test, label=y_test, reference=dtrain)\n",
    "    clf = lgb.train(\n",
    "        best_params,\n",
    "        dtrain,\n",
    "        valid_sets=[dtest],\n",
    "        num_boost_round=study.best_trial.user_attrs.get(\"num_boost_round\", 1000),\n",
    "    )\n",
    "\n",
    "    # Explicitly release resources\n",
    "    del dtrain, dtest, clf\n",
    "    gc.collect()\n",
    "\n",
    "    # Store results\n",
    "    return {\n",
    "        \"dataset\": key,\n",
    "        \"best_params\": best_params,\n",
    "    }\n",
    "\n",
    "\n",
    "# Use joblib to parallelize processing of each dataset, up to 6 tasks in parallel\n",
    "results = Parallel(n_jobs=6, verbose=30)(\n",
    "    delayed(optimize_and_evaluate)(key) for key in train_test_data.keys()\n",
    ")\n",
    "\n",
    "# Convert results to DataFrame and print\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eee3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"lgbresults.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b16e0",
   "metadata": {},
   "source": [
    "From the cell below, it can be run independently. This is the compare analysis section. Including SHAP and Model Performance Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc503a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Read the original dataset\n",
    "class_df = pd.read_csv(\"sampleddata/class_df.csv\", index_col=0)\n",
    "\n",
    "# Define sample levels and sampling methods\n",
    "sample_levels = [1000, 5000, 10000, 20000]\n",
    "\n",
    "sampling_methods = [\n",
    "    \"BalancedSampling_sampled_data\",\n",
    "    \"clhs_sampled_data\",\n",
    "    \"FCMtp1sampled_data\",\n",
    "    \"FCMtp2sampled_data\",\n",
    "    \"FSCS_sampled_data\",\n",
    "    \"kmeans_sampled_data\",\n",
    "]\n",
    "\n",
    "# Define covariates and target variable\n",
    "covariates = [\n",
    "    \"elevation\",\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"horizontal_distance_to_hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\",\n",
    "    \"Hillshade_Noon\",\n",
    "    \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\",\n",
    "]\n",
    "target = \"class\"\n",
    "\n",
    "# Store training and testing sets\n",
    "train_test_data = {}\n",
    "\n",
    "# Iterate through each sample level and sampling method\n",
    "for level in sample_levels:\n",
    "    for t in range(1, 21):\n",
    "        for method in sampling_methods:\n",
    "            sample_file = f\"sampleddata/combined_samples/{method}_{level}_set_{t}.csv\"\n",
    "\n",
    "            if not os.path.exists(sample_file):\n",
    "                print(f\"File {sample_file} does not exist. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            sample_data = pd.read_csv(sample_file, index_col=0)\n",
    "            X_train = sample_data[covariates]\n",
    "            y_train = sample_data[target]\n",
    "            train_indices = sample_data.index\n",
    "\n",
    "            # Generate the corresponding test set\n",
    "            test_data = class_df.drop(train_indices)\n",
    "            X_test = test_data[covariates]\n",
    "            y_test = test_data[target]\n",
    "\n",
    "            # Store training and testing sets\n",
    "            train_test_data[f\"{method}_{level}_set_{t}\"] = (\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_test,\n",
    "                y_test,\n",
    "            )\n",
    "\n",
    "# Generate SRS sample subsets\n",
    "for level in sample_levels:\n",
    "    for t in range(1, 21):\n",
    "        # Simple random sampling\n",
    "        srs_sample = resample(class_df, n_samples=level, random_state=42 + t)\n",
    "        X_train = srs_sample[covariates]\n",
    "        y_train = srs_sample[target]\n",
    "        train_indices = srs_sample.index\n",
    "\n",
    "        # Generate the corresponding test set\n",
    "        test_data = class_df.drop(train_indices)\n",
    "        X_test = test_data[covariates]\n",
    "        y_test = test_data[target]\n",
    "\n",
    "        # Store training and testing sets\n",
    "        train_test_data[f\"SRS_sampled_df_{level}_set_{t}\"] = (\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "        )\n",
    "\n",
    "print(\"All training and testing sets generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5908e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unnecessary variables to free memory\n",
    "del (\n",
    "    class_df,\n",
    "    sample_data,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    train_indices,\n",
    "    test_data,\n",
    "    srs_sample,\n",
    ")\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c484c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the saved DataFrame file\n",
    "results_df = pd.read_csv(\"lgbresults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e09ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from joblib import Parallel, delayed\n",
    "import fasttreeshap\n",
    "\n",
    "# Create a dictionary to store models\n",
    "models = {}\n",
    "\n",
    "# Specify the save path\n",
    "save_path = \"codepart/LGBpkl\"\n",
    "# Ensure the save path exists\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def process_dataset(index, row):\n",
    "    dataset = row[\"dataset\"]\n",
    "\n",
    "    best_params = row[\"best_params\"]\n",
    "    # best_params = eval(best_params)  # Already a dict, no need to convert\n",
    "\n",
    "    # Get the corresponding dataset\n",
    "    X_train, y_train, X_test, y_test = train_test_data[dataset]\n",
    "\n",
    "    # Use LabelEncoder to convert y_train and y_test to integer variables\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dtest = lgb.Dataset(X_test, label=y_test, reference=dtrain)\n",
    "\n",
    "    # File paths\n",
    "    model_filename = os.path.join(save_path, f\"{dataset}_model.pkl\")\n",
    "    shap_values_filename_v2 = os.path.join(save_path, f\"{dataset}_shap_values_v2.pkl\")\n",
    "    shap_interaction_values_filename_v1 = os.path.join(\n",
    "        save_path, f\"{dataset}_shap_interaction_values_v1.pkl\"\n",
    "    )\n",
    "    shap_explainer_v2_filename = os.path.join(\n",
    "        save_path, f\"{dataset}_shap_explainer_v2.pkl\"\n",
    "    )\n",
    "    shap_explainer_v1_filename = os.path.join(\n",
    "        save_path, f\"{dataset}_shap_explainer_v1.pkl\"\n",
    "    )\n",
    "\n",
    "    # Check if files already exist\n",
    "    if (\n",
    "        os.path.exists(model_filename)\n",
    "        and os.path.exists(shap_values_filename_v2)\n",
    "        and os.path.exists(shap_interaction_values_filename_v1)\n",
    "    ):\n",
    "        print(f\"{dataset} has already been processed, loading files directly.\")\n",
    "        clf = joblib.load(model_filename)\n",
    "        shap_values_v2 = joblib.load(shap_values_filename_v2)\n",
    "        shap_interaction_values_v1 = joblib.load(shap_interaction_values_filename_v1)\n",
    "        shap_explainer_v2 = joblib.load(shap_explainer_v2_filename)\n",
    "        shap_explainer_v1 = joblib.load(shap_explainer_v1_filename)\n",
    "\n",
    "    else:\n",
    "        # Train the model with the best hyperparameters\n",
    "        clf = lgb.train(\n",
    "            best_params,\n",
    "            dtrain,\n",
    "            valid_sets=[dtest],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=10)],\n",
    "        )\n",
    "\n",
    "        # Save the trained model\n",
    "        joblib.dump(clf, model_filename, compress=9)\n",
    "\n",
    "        # Perform SHAP analysis, use X_train to analyze feature importance and interaction importance\n",
    "        shap_explainer_v2 = fasttreeshap.TreeExplainer(clf, algorithm=\"v2\", n_jobs=-1)\n",
    "        shap_values_v2 = shap_explainer_v2(X_train).values\n",
    "\n",
    "        shap_explainer_v1 = fasttreeshap.TreeExplainer(clf, algorithm=\"v1\", n_jobs=-1)\n",
    "        shap_interaction_values_v1 = shap_explainer_v1(\n",
    "            X_train, interactions=True\n",
    "        ).values\n",
    "\n",
    "        # Save SHAP values\n",
    "        joblib.dump(shap_values_v2, shap_values_filename_v2, compress=9)\n",
    "        joblib.dump(\n",
    "            shap_interaction_values_v1, shap_interaction_values_filename_v1, compress=9\n",
    "        )\n",
    "\n",
    "        # Save SHAP explainers\n",
    "        joblib.dump(shap_explainer_v2, shap_explainer_v2_filename, compress=9)\n",
    "        joblib.dump(shap_explainer_v1, shap_explainer_v1_filename, compress=9)\n",
    "\n",
    "    # Final print statement\n",
    "    print(f\"Finished {dataset}\")\n",
    "\n",
    "\n",
    "# Use joblib to process in parallel and show progress bar\n",
    "with Parallel(n_jobs=-1, verbose=50) as parallel:\n",
    "    parallel(\n",
    "        delayed(process_dataset)(index, row) for index, row in results_df.iterrows()\n",
    "    )\n",
    "\n",
    "print(\"All datasets have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# Specify the save path\n",
    "save_path = \"codepart/LGBpkl\"\n",
    "# Ensure the save path exists\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def calculate_roc(index, row):\n",
    "    dataset = row[\"dataset\"]\n",
    "\n",
    "    # Get the corresponding dataset\n",
    "    X_train, y_train, X_test, y_test = train_test_data[dataset]\n",
    "\n",
    "    # Use LabelEncoder to convert y_train and y_test to integer variables\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "    # File path\n",
    "    model_filename = os.path.join(save_path, f\"{dataset}_model.pkl\")\n",
    "\n",
    "    # Check if the model file already exists\n",
    "    if os.path.exists(model_filename):\n",
    "        print(f\"{dataset} has already been processed, loading file directly.\")\n",
    "        clf = joblib.load(model_filename)\n",
    "\n",
    "        # Calculate ROC AUC score\n",
    "        y_pred_proba = clf.predict(\n",
    "            X_test\n",
    "        )  # Predict on the test set, return probability for each class\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "        if len(np.unique(y_test)) == 2:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class=\"ovr\")\n",
    "\n",
    "        # Calculate accuracy and F1 score\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "        # Return results\n",
    "        return index, roc_auc, accuracy, f1\n",
    "    else:\n",
    "        print(f\"Model file for {dataset} does not exist, skipping.\")\n",
    "        return index, None, None, None\n",
    "\n",
    "\n",
    "# Use joblib to process in parallel\n",
    "results = Parallel(n_jobs=-1, verbose=50)(\n",
    "    delayed(calculate_roc)(index, row) for index, row in results_df.iterrows()\n",
    ")\n",
    "\n",
    "# Update results_df\n",
    "for index, roc_auc, accuracy, f1 in results:\n",
    "    if roc_auc is not None:\n",
    "        results_df.loc[index, \"roc_auc\"] = roc_auc\n",
    "        results_df.loc[index, \"test_accuracy\"] = accuracy\n",
    "        results_df.loc[index, \"test_f1\"] = f1\n",
    "\n",
    "print(\"ROC AUC, accuracy, and F1 score calculation for all datasets is complete.\")\n",
    "results_df.to_csv(\"lgbresults_icluROCAUC.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09312722",
   "metadata": {},
   "source": [
    "From the cell below, it can be run independently. Figure drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605daae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the saved DataFrame file\n",
    "results_df = pd.read_csv(\"lgbresults_icluROCAUC.csv\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a new variable to store the extracted sampling method and dataset level\n",
    "results_with_methods = results_df.copy()\n",
    "results_with_methods[\"sampling_method\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[0]\n",
    ")\n",
    "results_with_methods[\"dataset_level\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[-3]\n",
    ")\n",
    "\n",
    "# Set font size\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "# Draw boxplots\n",
    "plt.figure(figsize=(10, 25))  # Adjust figure width\n",
    "\n",
    "# Draw boxplot for accuracy\n",
    "plt.subplot(3, 1, 1)\n",
    "sns.boxplot(\n",
    "    x=\"sampling_method\",\n",
    "    y=\"test_accuracy\",\n",
    "    hue=\"dataset_level\",\n",
    "    data=results_with_methods,\n",
    "    width=0.6,\n",
    ")\n",
    "plt.title(\"Accuracy Boxplot\", fontsize=20)\n",
    "plt.xlabel(\"Sampling Method\", fontsize=18, labelpad=20)\n",
    "plt.ylabel(\"Test Accuracy\", fontsize=18, labelpad=20)\n",
    "plt.grid(True, which=\"major\", axis=\"y\", linestyle=\"-\", linewidth=0.6)\n",
    "plt.grid(True, which=\"minor\", axis=\"y\", linestyle=\"--\", linewidth=0.25)\n",
    "plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.05))\n",
    "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.01))\n",
    "plt.legend(title=\"Dataset Level\")  # Add legend\n",
    "\n",
    "# Draw boxplot for F1 score\n",
    "plt.subplot(3, 1, 2)\n",
    "sns.boxplot(\n",
    "    x=\"sampling_method\", y=\"test_f1\", hue=\"dataset_level\", data=results_with_methods\n",
    ")\n",
    "plt.title(\"F1 Score Boxplot\", fontsize=20)\n",
    "plt.xlabel(\"Sampling Method\", fontsize=18, labelpad=20)\n",
    "plt.ylabel(\"Test F1 Score\", fontsize=18, labelpad=20)\n",
    "plt.grid(True, which=\"major\", axis=\"y\", linestyle=\"-\", linewidth=0.6)\n",
    "plt.grid(True, which=\"minor\", axis=\"y\", linestyle=\"--\", linewidth=0.25)\n",
    "plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.05))\n",
    "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.01))\n",
    "plt.legend(title=\"Dataset Level\")  # Add legend\n",
    "\n",
    "# Draw boxplot for ROC-AUC\n",
    "plt.subplot(3, 1, 3)\n",
    "sns.boxplot(\n",
    "    x=\"sampling_method\", y=\"roc_auc\", hue=\"dataset_level\", data=results_with_methods\n",
    ")\n",
    "plt.title(\"ROC-AUC Boxplot\", fontsize=20)\n",
    "plt.xlabel(\"Sampling Method\", fontsize=18, labelpad=20)\n",
    "plt.ylabel(\"ROC-AUC\", fontsize=18, labelpad=20)\n",
    "plt.grid(True, which=\"major\", axis=\"y\", linestyle=\"-\", linewidth=0.6)\n",
    "plt.grid(True, which=\"minor\", axis=\"y\", linestyle=\"--\", linewidth=0.25)\n",
    "plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.05))\n",
    "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.01))\n",
    "plt.legend(title=\"Dataset Level\")  # Add legend\n",
    "\n",
    "# Specify the X-axis labels from left to right\n",
    "sampling_methods = [\n",
    "    \"Balance\\nSampling\",\n",
    "    \"CLHS\",\n",
    "    \"FCM\\nClu = class\",\n",
    "    \"FCM\\nClu = level\",\n",
    "    \"FSCS\",\n",
    "    \"K-means\\nClu = class\",\n",
    "    \"SRS\",\n",
    "]\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.xticks(ticks=range(len(sampling_methods)), labels=sampling_methods)\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.xticks(ticks=range(len(sampling_methods)), labels=sampling_methods)\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.xticks(ticks=range(len(sampling_methods)), labels=sampling_methods)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"lgbresults_plot.jpg\", format=\"jpg\", dpi=800, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the median of R and RMSLE for each sampling method\n",
    "median_metrics = (\n",
    "    results_with_methods.groupby([\"sampling_method\", \"dataset_level\"])[\n",
    "        [\"test_accuracy\", \"test_f1\", \"roc_auc\"]\n",
    "    ]\n",
    "    .median()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Print median results\n",
    "print(median_metrics)\n",
    "\n",
    "\n",
    "# Calculate the percentage difference of each method relative to SRS of the same level\n",
    "def calculate_percentage_difference(group):\n",
    "    srs_values = group[group[\"sampling_method\"] == \"SRS\"]\n",
    "    if srs_values.empty:\n",
    "        return group.set_index(\"sampling_method\") * float(\"nan\")\n",
    "    srs_values = srs_values.iloc[0][[\"test_accuracy\", \"test_f1\", \"roc_auc\"]]\n",
    "    return (\n",
    "        group.set_index(\"sampling_method\")[[\"test_accuracy\", \"test_f1\", \"roc_auc\"]]\n",
    "        / srs_values\n",
    "        - 1\n",
    "    ) * 100\n",
    "\n",
    "\n",
    "# Group by dataset_level and calculate the percentage difference for each method relative to SRS\n",
    "percentage_diff = (\n",
    "    median_metrics.groupby(\"dataset_level\")\n",
    "    .apply(calculate_percentage_difference)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge into one table\n",
    "combined_metrics = pd.merge(\n",
    "    median_metrics,\n",
    "    percentage_diff,\n",
    "    on=[\"sampling_method\", \"dataset_level\"],\n",
    "    suffixes=(\"\", \"_percentage_diff\"),\n",
    ")\n",
    "\n",
    "# Print the combined table results\n",
    "print(\"Combined table results:\")\n",
    "print(combined_metrics)\n",
    "combined_metrics.to_csv(\"lgbmedian_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
