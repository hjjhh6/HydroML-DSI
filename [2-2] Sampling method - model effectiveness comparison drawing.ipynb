{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Read CSV files\n",
    "lgb_metrics = pd.read_csv(\"lgbmedian_metrics.csv\")\n",
    "rf_metrics = pd.read_csv(\"rfmedian_metrics.csv\")\n",
    "\n",
    "# Add a column to indicate which model the data comes from\n",
    "lgb_metrics[\"model\"] = \"LGB\"\n",
    "rf_metrics[\"model\"] = \"RF\"\n",
    "\n",
    "# Concatenate the two tables vertically\n",
    "combined_metrics = pd.concat([rf_metrics, lgb_metrics], ignore_index=True)\n",
    "\n",
    "# Rearrange column order\n",
    "combined_metrics = combined_metrics[\n",
    "    [\"sampling_method\", \"dataset_level\", \"test_accuracy\", \"test_f1\", \"roc_auc\", \"model\"]\n",
    "]\n",
    "\n",
    "# Calculate the difference and relative percentage of each metric compared to RF+SRS at the same dataset level\n",
    "for metric in [\"test_accuracy\", \"test_f1\", \"roc_auc\"]:\n",
    "    srs_rf_values = (\n",
    "        combined_metrics[\n",
    "            (combined_metrics[\"sampling_method\"] == \"SRS\")\n",
    "            & (combined_metrics[\"model\"] == \"RF\")\n",
    "        ][[\"dataset_level\", metric]]\n",
    "        .set_index(\"dataset_level\")\n",
    "        .to_dict()[metric]\n",
    "    )\n",
    "\n",
    "    combined_metrics[f\"{metric}_diff\"] = combined_metrics.apply(\n",
    "        lambda row: row[metric] - srs_rf_values[row[\"dataset_level\"]],\n",
    "        axis=1,\n",
    "    )\n",
    "    combined_metrics[f\"{metric}_pct_diff\"] = combined_metrics.apply(\n",
    "        lambda row: (row[metric] - srs_rf_values[row[\"dataset_level\"]])\n",
    "        / srs_rf_values[row[\"dataset_level\"]]\n",
    "        * 100,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "# Create a new column combining sampling_method and model, with a line break after the model name\n",
    "combined_metrics[\"sampling_model\"] = (\n",
    "    combined_metrics[\"model\"] + \"\\n\" + combined_metrics[\"sampling_method\"]\n",
    ")\n",
    "\n",
    "# Remove the RF + SRS combination\n",
    "combined_metrics = combined_metrics[\n",
    "    ~(\n",
    "        (combined_metrics[\"sampling_method\"] == \"SRS\")\n",
    "        & (combined_metrics[\"model\"] == \"RF\")\n",
    "    )\n",
    "]\n",
    "\n",
    "# Specify the X-axis labels and corresponding sampling methods from left to right\n",
    "sampling_methods = [\n",
    "    \"BalancedSampling\",\n",
    "    \"clhs\",\n",
    "    \"FCMtp1sampled\",\n",
    "    \"FCMtp2sampled\",\n",
    "    \"FSCS\",\n",
    "    \"kmeans\",\n",
    "    \"SRS\",\n",
    "]\n",
    "\n",
    "# Corresponding X-axis labels\n",
    "sampling_labels = [\n",
    "    \"Balanced\\nSampling\",\n",
    "    \"CLHS\",\n",
    "    \"FCM\\nClu = class\",\n",
    "    \"FCM\\nClu = level\",\n",
    "    \"FSCS\",\n",
    "    \"K-means\\nClu = class\",\n",
    "    \"SRS\",\n",
    "]\n",
    "\n",
    "# Generate new X-axis label order\n",
    "sampling_model_order = []\n",
    "sampling_label_new = []\n",
    "for method, label in zip(sampling_methods, sampling_labels):\n",
    "    sampling_model_order.append(f\"RF\\n{method}\")\n",
    "    sampling_model_order.append(f\"LGB\\n{method}\")\n",
    "\n",
    "    sampling_label_new.append(f\"RF\\n{label}\")\n",
    "    sampling_label_new.append(f\"LGB\\n{label}\")\n",
    "\n",
    "# Generate new percentage label order\n",
    "sampling_modeldatalevel_order = []\n",
    "for method, label in zip(sampling_methods, sampling_labels):\n",
    "    for level in sorted(\n",
    "        combined_metrics[\"dataset_level\"].unique(),\n",
    "        key=lambda x: [1000, 5000, 10000, 20000].index(x),\n",
    "    ):\n",
    "        if method != \"SRS\":\n",
    "            sampling_modeldatalevel_order.append((f\"RF\\n{method}\", level))\n",
    "    for level in sorted(\n",
    "        combined_metrics[\"dataset_level\"].unique(),\n",
    "        key=lambda x: [1000, 5000, 10000, 20000].index(x),\n",
    "    ):\n",
    "        sampling_modeldatalevel_order.append((f\"LGB\\n{method}\", level))\n",
    "\n",
    "# Remove the RF + SRS label\n",
    "sampling_model_order.remove(\"RF\\nSRS\")\n",
    "sampling_label_new.remove(\"RF\\nSRS\")\n",
    "\n",
    "# Set font size\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "# Visualization\n",
    "metrics = [\"test_accuracy\", \"test_f1\", \"roc_auc\"]\n",
    "metric_titles = [\"Accuracy\", \"F1 Score\", \"ROC-AUC\"]\n",
    "\n",
    "# Visualize the metric improvement from RF+SRS to LGB/RF + each sampling method\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 22), sharey=False)\n",
    "for ax, metric, title in zip(axes, metrics, metric_titles):\n",
    "\n",
    "    bars = sns.barplot(\n",
    "        x=\"sampling_model\",\n",
    "        y=f\"{metric}_diff\",\n",
    "        hue=\"dataset_level\",\n",
    "        data=combined_metrics,\n",
    "        order=sampling_model_order,  # Set X-axis order\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Difference in {title} from RF+SRS to LGB/RF + Sampling Methods\", fontsize=16\n",
    "    )\n",
    "    ax.set_xlabel(\"Sampling Method + Model\", fontsize=15, labelpad=10)\n",
    "    ax.set_ylabel(f\"Difference in {title}\", fontsize=15, labelpad=10)\n",
    "    ax.legend(title=\"Dataset Level\", loc=\"lower left\", bbox_to_anchor=(0, 0))\n",
    "    ax.axhline(0, color=\"gray\", linestyle=\"--\")\n",
    "    ax.set_xticks(range(len(sampling_model_order)))\n",
    "    ax.set_xticklabels(sampling_label_new, fontsize=12)\n",
    "    ax.grid(True, which=\"major\", axis=\"y\", linestyle=\"-\", linewidth=0.6)\n",
    "    ax.grid(True, which=\"minor\", axis=\"y\", linestyle=\"--\", linewidth=0.25)\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.05))\n",
    "    ax.yaxis.set_minor_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "    # Add the current value at the end of each bar\n",
    "    texts = []\n",
    "    for p in bars.patches:\n",
    "        height = p.get_height()\n",
    "        diff_label = format(height, \".2f\")\n",
    "\n",
    "        # Find the corresponding {metric}_diff position\n",
    "        matching_row = combined_metrics[combined_metrics[f\"{metric}_diff\"] == height]\n",
    "\n",
    "        if not matching_row.empty:\n",
    "            pct_label = format(matching_row[f\"{metric}_pct_diff\"].values[0], \".1f\")\n",
    "        else:\n",
    "            pct_label = \"N/A\"\n",
    "\n",
    "        if diff_label != \"0.00\" and diff_label != \"-0.00\":\n",
    "            left = p.get_x() + p.get_width() / 2.0\n",
    "            if height > 0:\n",
    "                top = height\n",
    "                text = ax.text(\n",
    "                    left,\n",
    "                    top * 0.7,\n",
    "                    f\"{diff_label}\\n{pct_label}%\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=11,\n",
    "                )\n",
    "                texts.append(text)\n",
    "            else:\n",
    "                top = height\n",
    "                text = ax.text(\n",
    "                    left,\n",
    "                    top * 0.7,\n",
    "                    f\"{diff_label}\\n{pct_label}%\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"top\",\n",
    "                    fontsize=11,\n",
    "                )\n",
    "                texts.append(text)\n",
    "\n",
    "    adjust_text(texts, add_objects=bars, only_move=\"y\", ax=ax, max_move=(4, 4))\n",
    "\n",
    "    ax.axvspan(\n",
    "        sampling_model_order.index(\"LGB\\nSRS\") - 0.5,\n",
    "        sampling_model_order.index(\"LGB\\nSRS\") + 0.5,\n",
    "        facecolor=\"lightgray\",\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.15,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Read CSV files\n",
    "lgb_metrics = pd.read_csv(\"lgbKsatmedian_metrics.csv\")\n",
    "rf_metrics = pd.read_csv(\"rfKsatmedian_metrics.csv\")\n",
    "\n",
    "# Add a column to indicate which model the data comes from\n",
    "lgb_metrics[\"model\"] = \"LGB\"\n",
    "rf_metrics[\"model\"] = \"RF\"\n",
    "\n",
    "# Concatenate the two tables vertically\n",
    "combined_metrics = pd.concat([rf_metrics, lgb_metrics], ignore_index=True)\n",
    "\n",
    "# Rearrange column order\n",
    "combined_metrics = combined_metrics[\n",
    "    [\"sampling_method\", \"dataset_level\", \"r2\", \"rmsle\", \"model\"]\n",
    "]\n",
    "\n",
    "# Calculate the difference and relative percentage of each metric compared to RF+SRS at the same dataset level\n",
    "for metric in [\"r2\", \"rmsle\"]:\n",
    "    srs_rf_values = (\n",
    "        combined_metrics[\n",
    "            (combined_metrics[\"sampling_method\"] == \"SRS\")\n",
    "            & (combined_metrics[\"model\"] == \"RF\")\n",
    "        ][[\"dataset_level\", metric]]\n",
    "        .set_index(\"dataset_level\")\n",
    "        .to_dict()[metric]\n",
    "    )\n",
    "\n",
    "    combined_metrics[f\"{metric}_diff\"] = combined_metrics.apply(\n",
    "        lambda row: row[metric] - srs_rf_values[row[\"dataset_level\"]],\n",
    "        axis=1,\n",
    "    )\n",
    "    combined_metrics[f\"{metric}_pct_diff\"] = combined_metrics.apply(\n",
    "        lambda row: (row[metric] - srs_rf_values[row[\"dataset_level\"]])\n",
    "        / srs_rf_values[row[\"dataset_level\"]]\n",
    "        * 100,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "# Create a new column combining sampling_method and model, with a line break after the model name\n",
    "combined_metrics[\"sampling_model\"] = (\n",
    "    combined_metrics[\"model\"] + \"\\n\" + combined_metrics[\"sampling_method\"]\n",
    ")\n",
    "\n",
    "# Remove the RF + SRS combination\n",
    "combined_metrics = combined_metrics[\n",
    "    ~(\n",
    "        (combined_metrics[\"sampling_method\"] == \"SRS\")\n",
    "        & (combined_metrics[\"model\"] == \"RF\")\n",
    "    )\n",
    "]\n",
    "\n",
    "# Specify the X-axis labels and corresponding sampling methods from left to right\n",
    "sampling_methods = [\n",
    "    \"BalancedSampling\",\n",
    "    \"clhs\",\n",
    "    \"FCMtp2sampled\",\n",
    "    \"FSCS\",\n",
    "    \"SRS\",\n",
    "]\n",
    "\n",
    "# Corresponding X-axis labels\n",
    "sampling_labels = [\n",
    "    \"Balanced\\nSampling\",\n",
    "    \"CLHS\",\n",
    "    \"FCM\\nClu = level\",\n",
    "    \"FSCS\",\n",
    "    \"SRS\",\n",
    "]\n",
    "\n",
    "# Generate new X-axis label order\n",
    "sampling_model_order = []\n",
    "sampling_label_new = []\n",
    "for method, label in zip(sampling_methods, sampling_labels):\n",
    "    sampling_model_order.append(f\"RF\\n{method}\")\n",
    "    sampling_model_order.append(f\"LGB\\n{method}\")\n",
    "\n",
    "    sampling_label_new.append(f\"RF\\n{label}\")\n",
    "    sampling_label_new.append(f\"LGB\\n{label}\")\n",
    "\n",
    "# Generate new percentage label order\n",
    "sampling_modeldatalevel_order = []\n",
    "for method, label in zip(sampling_methods, sampling_labels):\n",
    "    for level in sorted(\n",
    "        combined_metrics[\"dataset_level\"].unique(),\n",
    "        key=lambda x: [1000, 5000, 10000].index(x),\n",
    "    ):\n",
    "        if method != \"SRS\":\n",
    "            sampling_modeldatalevel_order.append((f\"RF\\n{method}\", level))\n",
    "    for level in sorted(\n",
    "        combined_metrics[\"dataset_level\"].unique(),\n",
    "        key=lambda x: [1000, 5000, 10000].index(x),\n",
    "    ):\n",
    "        sampling_modeldatalevel_order.append((f\"LGB\\n{method}\", level))\n",
    "\n",
    "# Remove the RF + SRS label\n",
    "sampling_model_order.remove(\"RF\\nSRS\")\n",
    "sampling_label_new.remove(\"RF\\nSRS\")\n",
    "\n",
    "# Set font size\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "# Visualization\n",
    "metrics = [\"r2\", \"rmsle\"]\n",
    "metric_titles = [\"R2\", \"RMSLE\"]\n",
    "\n",
    "# Visualize the metric improvement from RF+SRS to LGB/RF + each sampling method\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 22), sharey=False)\n",
    "for ax, metric, title in zip(axes, metrics, metric_titles):\n",
    "\n",
    "    bars = sns.barplot(\n",
    "        x=\"sampling_model\",\n",
    "        y=f\"{metric}_diff\",\n",
    "        hue=\"dataset_level\",\n",
    "        data=combined_metrics,\n",
    "        order=sampling_model_order,  # Set X-axis order\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Difference in {title} from RF+SRS to LGB/RF + Sampling Methods\", fontsize=16\n",
    "    )\n",
    "    ax.set_xlabel(\"Sampling Method + Model\", fontsize=15, labelpad=10)\n",
    "    ax.set_ylabel(f\"Difference in {title}\", fontsize=15, labelpad=10)\n",
    "    ax.legend(title=\"Dataset Level\", loc=\"lower left\", bbox_to_anchor=(0, 0))\n",
    "    ax.axhline(0, color=\"gray\", linestyle=\"--\")\n",
    "    ax.set_xticks(range(len(sampling_model_order)))\n",
    "    ax.set_xticklabels(sampling_label_new, fontsize=15)\n",
    "    ax.grid(True, which=\"major\", axis=\"y\", linestyle=\"-\", linewidth=0.6)\n",
    "    ax.grid(True, which=\"minor\", axis=\"y\", linestyle=\"--\", linewidth=0.25)\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.05))\n",
    "    ax.yaxis.set_minor_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "    # Add the current value at the end of each bar\n",
    "\n",
    "    texts = []\n",
    "    for p in bars.patches:\n",
    "        height = p.get_height()\n",
    "        diff_label = format(height, \".2f\")\n",
    "\n",
    "        # Find the corresponding {metric}_diff position\n",
    "        matching_row = combined_metrics[combined_metrics[f\"{metric}_diff\"] == height]\n",
    "\n",
    "        if not matching_row.empty:\n",
    "            pct_label = format(matching_row[f\"{metric}_pct_diff\"].values[0], \".1f\")\n",
    "        else:\n",
    "            pct_label = \"N/A\"\n",
    "\n",
    "        if diff_label != \"0.00\" and diff_label != \"-0.00\":\n",
    "            left = p.get_x() + p.get_width() / 2.0\n",
    "            if height > 0:\n",
    "                top = height\n",
    "                text = ax.text(\n",
    "                    left,\n",
    "                    top,\n",
    "                    f\"{diff_label}\\n{pct_label}%\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=11,\n",
    "                )\n",
    "                texts.append(text)\n",
    "            else:\n",
    "                top = height\n",
    "                text = ax.text(\n",
    "                    left,\n",
    "                    top,\n",
    "                    f\"{diff_label}\\n{pct_label}%\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"top\",\n",
    "                    fontsize=11,\n",
    "                )\n",
    "                texts.append(text)\n",
    "\n",
    "    adjust_text(texts, add_objects=bars, only_move=\"y+\", ax=ax, max_move=(0.1, 0.1))\n",
    "\n",
    "    if metric == \"rmsle\":\n",
    "        ax.invert_yaxis()  # Invert y-axis\n",
    "\n",
    "    ax.axvspan(\n",
    "        sampling_model_order.index(\"LGB\\nSRS\") - 0.5,\n",
    "        sampling_model_order.index(\"LGB\\nSRS\") + 0.5,\n",
    "        facecolor=\"lightgray\",\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.15,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Ksatmetric.jpg\", format=\"jpg\", dpi=800, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a778b",
   "metadata": {},
   "source": [
    "From the cell below, it can be run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All training and testing sets generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Data loading and train/test set construction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Read the original dataset\n",
    "class_df = pd.read_csv(\"sampleddata/class_df.csv\", index_col=0)\n",
    "\n",
    "# Define sample sizes and sampling methods\n",
    "sample_levels = [1000, 5000, 10000, 20000]\n",
    "\n",
    "sampling_methods = [\n",
    "    \"BalancedSampling_sampled_data\",\n",
    "    \"clhs_sampled_data\",\n",
    "    \"FCMtp1sampled_data\",\n",
    "    \"FCMtp2sampled_data\",\n",
    "    \"FSCS_sampled_data\",\n",
    "    \"kmeans_sampled_data\",\n",
    "]\n",
    "\n",
    "# Define covariates and target variable\n",
    "covariates = [\n",
    "    \"elevation\",\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"horizontal_distance_to_hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\",\n",
    "    \"Hillshade_Noon\",\n",
    "    \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\",\n",
    "]\n",
    "target = \"class\"\n",
    "\n",
    "# Store train and test sets\n",
    "train_test_data = {}\n",
    "\n",
    "# Loop through each sample size and sampling method\n",
    "for level in sample_levels:\n",
    "    for t in range(1, 21):\n",
    "        for method in sampling_methods:\n",
    "            sample_file = f\"sampleddata/combined_samples/{method}_{level}_set_{t}.csv\"\n",
    "\n",
    "            if not os.path.exists(sample_file):\n",
    "                print(f\"File {sample_file} does not exist. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            sample_data = pd.read_csv(sample_file, index_col=0)\n",
    "            X_train = sample_data[covariates]\n",
    "            y_train = sample_data[target]\n",
    "            train_indices = sample_data.index\n",
    "\n",
    "            # Generate the corresponding test set\n",
    "            test_data = class_df.drop(train_indices)\n",
    "            X_test = test_data[covariates]\n",
    "            y_test = test_data[target]\n",
    "\n",
    "            # Store train and test sets\n",
    "            train_test_data[f\"{method}_{level}_set_{t}\"] = (\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_test,\n",
    "                y_test,\n",
    "            )\n",
    "\n",
    "# Generate SRS sample subsets\n",
    "for level in sample_levels:\n",
    "    for t in range(1, 21):\n",
    "        # Simple random sampling\n",
    "        srs_sample = resample(class_df, n_samples=level, random_state=42 + t)\n",
    "        X_train = srs_sample[covariates]\n",
    "        y_train = srs_sample[target]\n",
    "        train_indices = srs_sample.index\n",
    "\n",
    "        # Generate the corresponding test set\n",
    "        test_data = class_df.drop(train_indices)\n",
    "        X_test = test_data[covariates]\n",
    "        y_test = test_data[target]\n",
    "\n",
    "        # Store train and test sets\n",
    "        train_test_data[f\"SRS_sampled_df_{level}_set_{t}\"] = (\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "        )\n",
    "\n",
    "print(\"All training and testing sets generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved DataFrame files\n",
    "results_df_rf = pd.read_csv(\"rfresults_icluROCAUC.csv\")\n",
    "results_df_lgb = pd.read_csv(\"lgbresults_icluROCAUC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest comparison: SRS vs FSCS at 20000 sample size for multi-class problem\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import fasttreeshap\n",
    "\n",
    "# Create a new variable to store extracted sampling method and dataset level\n",
    "results_with_methods = results_df_rf.copy()\n",
    "\n",
    "results_with_methods[\"sampling_method\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[0]\n",
    ")\n",
    "results_with_methods[\"dataset_level\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[-3]\n",
    ")\n",
    "\n",
    "results_with_methods[\"set_number\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[-1]\n",
    ")\n",
    "\n",
    "# Calculate the average of three metrics\n",
    "results_with_methods[\"average_metric\"] = results_with_methods[\n",
    "    [\"test_accuracy\", \"test_f1\", \"roc_auc\"]\n",
    "].mean(axis=1)\n",
    "\n",
    "# Find the median-performing SRS result at 20000 level\n",
    "sorted_results_srs = results_with_methods[\n",
    "    (results_with_methods[\"sampling_method\"] == \"SRS\")\n",
    "    & (results_with_methods[\"dataset_level\"] == \"20000\")\n",
    "].sort_values(by=\"average_metric\", ascending=False)\n",
    "\n",
    "# Find the median-performing FSCS result at 20000 level\n",
    "sorted_results_fscs = results_with_methods[\n",
    "    (results_with_methods[\"sampling_method\"] == \"FSCS\")\n",
    "    & (results_with_methods[\"dataset_level\"] == \"20000\")\n",
    "].sort_values(by=\"average_metric\", ascending=False)\n",
    "\n",
    "# Get the median value\n",
    "median_metric_srs = sorted_results_srs[\"average_metric\"].median()\n",
    "median_metric = sorted_results_fscs[\"average_metric\"].median()\n",
    "\n",
    "# Find the result closest to the median\n",
    "sorted_results_srs[\"metric_diff\"] = (\n",
    "    sorted_results_srs[\"average_metric\"] - median_metric\n",
    ").abs()\n",
    "middle_result_srs = sorted_results_srs.loc[sorted_results_srs[\"metric_diff\"].idxmin()]\n",
    "\n",
    "sorted_results_fscs[\"metric_diff\"] = (\n",
    "    sorted_results_fscs[\"average_metric\"] - median_metric\n",
    ").abs()\n",
    "middle_result_fscs = sorted_results_fscs.loc[\n",
    "    sorted_results_fscs[\"metric_diff\"].idxmin()\n",
    "]\n",
    "\n",
    "# Get the set number\n",
    "set_number_srs = middle_result_srs[\"set_number\"]\n",
    "set_number_fscs = middle_result_fscs[\"set_number\"]\n",
    "\n",
    "# Build file paths\n",
    "file_path_srs = (\n",
    "    f\"codepart\\\\RFpkl\\\\SRS_sampled_df_20000_set_{set_number_srs}_shap_values_v2.pkl\"\n",
    ")\n",
    "model_path_srs = f\"codepart\\\\RFpkl\\\\SRS_sampled_df_20000_set_{set_number_srs}_model.pkl\"\n",
    "\n",
    "# Load pkl files\n",
    "shap_values_srs_rf = joblib.load(file_path_srs)\n",
    "clf_model_srs_rf = joblib.load(model_path_srs)\n",
    "\n",
    "# Build file paths\n",
    "file_path_fscs = (\n",
    "    f\"codepart\\\\RFpkl\\\\FSCS_sampled_data_20000_set_{set_number_fscs}_shap_values_v2.pkl\"\n",
    ")\n",
    "model_path_fscs = (\n",
    "    f\"codepart\\\\RFpkl\\\\FSCS_sampled_data_20000_set_{set_number_fscs}_model.pkl\"\n",
    ")\n",
    "\n",
    "# Load pkl files\n",
    "shap_values_fscs_rf = joblib.load(file_path_fscs)\n",
    "clf_model_fscs_rf = joblib.load(model_path_fscs)\n",
    "\n",
    "# Build dataset keys\n",
    "dataset_srs = f\"SRS_sampled_df_20000_set_{set_number_srs}\"\n",
    "dataset_fscs = f\"FSCS_sampled_data_20000_set_{set_number_srs}\"\n",
    "\n",
    "# Get the corresponding datasets\n",
    "X_train_srs_rf, y_train_srs_rf, X_test_srs_rf, y_test_srs_rf = train_test_data[\n",
    "    dataset_srs\n",
    "]\n",
    "X_train_fscs_rf, y_train_fscs_rf, X_test_fscs_rf, y_test_fscs_rf = train_test_data[\n",
    "    dataset_fscs\n",
    "]\n",
    "\n",
    "y_pred_srs_rf = clf_model_srs_rf.predict(X_test_srs_rf)\n",
    "y_pred_fscs_rf = clf_model_fscs_rf.predict(X_test_fscs_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91691e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM comparison: SRS vs FSCS at 20000 sample size for multi-class problem\n",
    "import joblib\n",
    "import fasttreeshap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a new variable to store extracted sampling method and dataset level\n",
    "results_with_methods = results_df_lgb.copy()\n",
    "\n",
    "results_with_methods[\"sampling_method\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[0]\n",
    ")\n",
    "results_with_methods[\"dataset_level\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[-3]\n",
    ")\n",
    "\n",
    "results_with_methods[\"set_number\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[-1]\n",
    ")\n",
    "\n",
    "# Calculate the average of three metrics\n",
    "results_with_methods[\"average_metric\"] = results_with_methods[\n",
    "    [\"test_accuracy\", \"test_f1\", \"roc_auc\"]\n",
    "].mean(axis=1)\n",
    "\n",
    "# Find the median-performing SRS result at 20000 level\n",
    "sorted_results_srs = results_with_methods[\n",
    "    (results_with_methods[\"sampling_method\"] == \"SRS\")\n",
    "    & (results_with_methods[\"dataset_level\"] == \"20000\")\n",
    "].sort_values(by=\"average_metric\", ascending=False)\n",
    "\n",
    "# Find the median-performing FSCS result at 20000 level\n",
    "sorted_results_fscs = results_with_methods[\n",
    "    (results_with_methods[\"sampling_method\"] == \"FSCS\")\n",
    "    & (results_with_methods[\"dataset_level\"] == \"20000\")\n",
    "].sort_values(by=\"average_metric\", ascending=False)\n",
    "\n",
    "# Get the median value\n",
    "median_metric_srs = sorted_results_srs[\"average_metric\"].median()\n",
    "median_metric = sorted_results_fscs[\"average_metric\"].median()\n",
    "\n",
    "# Find the result closest to the median\n",
    "sorted_results_srs[\"metric_diff\"] = (\n",
    "    sorted_results_srs[\"average_metric\"] - median_metric\n",
    ").abs()\n",
    "middle_result_srs = sorted_results_srs.loc[sorted_results_srs[\"metric_diff\"].idxmin()]\n",
    "\n",
    "sorted_results_fscs[\"metric_diff\"] = (\n",
    "    sorted_results_fscs[\"average_metric\"] - median_metric\n",
    ").abs()\n",
    "middle_result_fscs = sorted_results_fscs.loc[\n",
    "    sorted_results_fscs[\"metric_diff\"].idxmin()\n",
    "]\n",
    "\n",
    "# Get the set number\n",
    "set_number_srs = middle_result_srs[\"set_number\"]\n",
    "set_number_fscs = middle_result_fscs[\"set_number\"]\n",
    "\n",
    "# Build model file paths\n",
    "file_path_srs = f\"codepart\\\\LGBpkl\\\\SRS_sampled_df_20000_set_{set_number_srs}_model.pkl\"\n",
    "clf_srs = joblib.load(file_path_srs)\n",
    "file_path_fscs = (\n",
    "    f\"codepart\\\\LGBpkl\\\\FSCS_sampled_data_20000_set_{set_number_fscs}_model.pkl\"\n",
    ")\n",
    "clf_fscs = joblib.load(file_path_fscs)\n",
    "\n",
    "# Build dataset keys\n",
    "dataset_srs = f\"SRS_sampled_df_20000_set_{set_number_srs}\"\n",
    "dataset_fscs = f\"FSCS_sampled_data_20000_set_{set_number_fscs}\"\n",
    "\n",
    "# Get the corresponding datasets\n",
    "X_train_srs_lgb, y_train_srs_lgb, X_test_srs_lgb, y_test_srs_lgb = train_test_data[\n",
    "    dataset_srs\n",
    "]\n",
    "X_train_fscs_lgb, y_train_fscs_lgb, X_test_fscs_lgb, y_test_fscs_lgb = train_test_data[\n",
    "    dataset_fscs\n",
    "]\n",
    "\n",
    "# Calculate SHAP values\n",
    "# It can be modified to read directly, but fasttreeshap is also fast.\n",
    "shap_explainer_srs = fasttreeshap.TreeExplainer(\n",
    "    clf_srs, algorithm=\"v0\", n_jobs=-1, shortcut=True\n",
    ")\n",
    "shap_values_srs_lgb = shap_explainer_srs(X_train_srs_lgb).values\n",
    "\n",
    "shap_explainer_fscs = fasttreeshap.TreeExplainer(\n",
    "    clf_fscs, algorithm=\"v0\", n_jobs=-1, shortcut=True\n",
    ")\n",
    "shap_values_fscs_lgb = shap_explainer_fscs(X_train_fscs_lgb).values\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_train_srs_lgb = le.fit_transform(y_train_srs_lgb)\n",
    "y_train_srs_lgb = le.inverse_transform(y_train_srs_lgb)\n",
    "\n",
    "# Predict test set labels for SRS\n",
    "y_pred_srs_lgb_proba = clf_srs.predict(X_test_srs_lgb)\n",
    "y_pred_srs_lgb = np.argmax(y_pred_srs_lgb_proba, axis=1)\n",
    "y_pred_srs_lgb = le.inverse_transform(y_pred_srs_lgb)\n",
    "\n",
    "# Predict test set labels for FSCS\n",
    "y_pred_fscs_lgb_proba = clf_fscs.predict(X_test_fscs_lgb)\n",
    "y_pred_fscs_lgb = np.argmax(y_pred_fscs_lgb_proba, axis=1)\n",
    "y_pred_fscs_lgb = le.inverse_transform(y_pred_fscs_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfda653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of SHAP feature importance for each class\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Calculate the mean absolute SHAP value for each feature in each class\n",
    "shap_values_srs_rf_abs_mean = np.mean(np.abs(shap_values_srs_rf), axis=0)\n",
    "shap_values_fscs_rf_abs_mean = np.mean(np.abs(shap_values_fscs_rf), axis=0)\n",
    "shap_values_srs_lgb_abs_mean = np.mean(np.abs(shap_values_srs_lgb), axis=0)\n",
    "shap_values_fscs_lgb_abs_mean = np.mean(np.abs(shap_values_fscs_lgb), axis=0)\n",
    "shap_values_srs_rf_abs_mean = shap_values_srs_rf_abs_mean / np.sum(\n",
    "    shap_values_srs_rf_abs_mean\n",
    ")\n",
    "shap_values_fscs_rf_abs_mean = shap_values_fscs_rf_abs_mean / np.sum(\n",
    "    shap_values_fscs_rf_abs_mean\n",
    ")\n",
    "shap_values_srs_lgb_abs_mean = shap_values_srs_lgb_abs_mean / np.sum(\n",
    "    shap_values_srs_lgb_abs_mean\n",
    ")\n",
    "shap_values_fscs_lgb_abs_mean = shap_values_fscs_lgb_abs_mean / np.sum(\n",
    "    shap_values_fscs_lgb_abs_mean\n",
    ")\n",
    "\n",
    "# Class names\n",
    "class_names = [\n",
    "    \"Aspen\",\n",
    "    \"Cottonwood Willow\",\n",
    "    \"Douglas Fir\",\n",
    "    \"Krummholz\",\n",
    "    \"Lodgepole Pine\",\n",
    "    \"Ponderosa Pine\",\n",
    "    \"Spruce Fir\",\n",
    "]\n",
    "\n",
    "# Manually specify x-axis labels\n",
    "feature_names = [\n",
    "    \"Elevation\",\n",
    "    \"Aspect\",\n",
    "    \"Slope\",\n",
    "    \"Horizontal Distance\\nto Hydrology\",\n",
    "    \"Vertical Distance\\nto Hydrology\",\n",
    "    \"Horizontal Distance\\nto Roadways\",\n",
    "    \"Hillshade 9am\",\n",
    "    \"Hillshade Noon\",\n",
    "    \"Hillshade 3pm\",\n",
    "    \"Horizontal Distance\\nto Fire Points\",\n",
    "]\n",
    "\n",
    "# Add sampling method names to feature names\n",
    "feature_names_srs_rf = [f\"RF+SRS-{feature}\" for feature in feature_names]\n",
    "feature_names_fscs_rf = [f\"RF+FSCS-{feature}\" for feature in feature_names]\n",
    "feature_names_srs_lgb = [f\"LGB+SRS-{feature}\" for feature in feature_names]\n",
    "feature_names_fscs_lgb = [f\"LGB+FSCS-{feature}\" for feature in feature_names]\n",
    "\n",
    "# Interleave feature names\n",
    "combined_feature_names = []\n",
    "combined_shap_values_abs_mean = []\n",
    "\n",
    "for srs_rf_feature, fscs_rf_feature, srs_lgb_feature, fscs_lgb_feature in zip(\n",
    "    feature_names_srs_rf,\n",
    "    feature_names_fscs_rf,\n",
    "    feature_names_srs_lgb,\n",
    "    feature_names_fscs_lgb,\n",
    "):\n",
    "    combined_feature_names.append(srs_rf_feature)\n",
    "    combined_feature_names.append(fscs_rf_feature)\n",
    "    combined_feature_names.append(srs_lgb_feature)\n",
    "    combined_feature_names.append(fscs_lgb_feature)\n",
    "\n",
    "for srs_rf_shap, fscs_rf_shap, srs_lgb_shap, fscs_lgb_shap in zip(\n",
    "    shap_values_srs_rf_abs_mean,\n",
    "    shap_values_fscs_rf_abs_mean,\n",
    "    shap_values_srs_lgb_abs_mean,\n",
    "    shap_values_fscs_lgb_abs_mean,\n",
    "):\n",
    "    combined_shap_values_abs_mean.append(srs_rf_shap)\n",
    "    combined_shap_values_abs_mean.append(fscs_rf_shap)\n",
    "    combined_shap_values_abs_mean.append(srs_lgb_shap)\n",
    "    combined_shap_values_abs_mean.append(fscs_lgb_shap)\n",
    "\n",
    "combined_shap_values_abs_mean = np.array(combined_shap_values_abs_mean)\n",
    "\n",
    "# Plot stacked bar chart\n",
    "num_classes = shap_values_srs_rf.shape[2]\n",
    "num_features = len(feature_names)\n",
    "x = np.arange(num_features)\n",
    "width = 0.35  # Adjust width to fit four bar plots\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "bottom_srs_rf = np.zeros(num_features)\n",
    "bottom_fscs_rf = np.zeros(num_features)\n",
    "bottom_srs_lgb = np.zeros(num_features)\n",
    "bottom_fscs_lgb = np.zeros(num_features)\n",
    "\n",
    "# Plot SHAP values for SRS and FSCS\n",
    "for i in range(num_classes):\n",
    "    bar_srs_rf = ax.bar(\n",
    "        x - 3 * width / 4,\n",
    "        shap_values_srs_rf_abs_mean[:, i],\n",
    "        width / 2,\n",
    "        label=f\"{class_names[i]}\",\n",
    "        bottom=bottom_srs_rf,\n",
    "    )\n",
    "    bar_fscs_rf = ax.bar(\n",
    "        x - width / 4,\n",
    "        shap_values_fscs_rf_abs_mean[:, i],\n",
    "        width / 2,\n",
    "        bottom=bottom_fscs_rf,\n",
    "        color=[bar.get_facecolor() for bar in bar_srs_rf],\n",
    "    )\n",
    "    bar_srs_lgb = ax.bar(\n",
    "        x + width / 4,\n",
    "        shap_values_srs_lgb_abs_mean[:, i],\n",
    "        width / 2,\n",
    "        bottom=bottom_srs_lgb,\n",
    "        color=[bar.get_facecolor() for bar in bar_srs_rf],\n",
    "    )\n",
    "    bar_fscs_lgb = ax.bar(\n",
    "        x + 3 * width / 4,\n",
    "        shap_values_fscs_lgb_abs_mean[:, i],\n",
    "        width / 2,\n",
    "        bottom=bottom_fscs_lgb,\n",
    "        color=[bar.get_facecolor() for bar in bar_srs_rf],\n",
    "    )\n",
    "    bottom_srs_rf += shap_values_srs_rf_abs_mean[:, i]\n",
    "    bottom_fscs_rf += shap_values_fscs_rf_abs_mean[:, i]\n",
    "    bottom_srs_lgb += shap_values_srs_lgb_abs_mean[:, i]\n",
    "    bottom_fscs_lgb += shap_values_fscs_lgb_abs_mean[:, i]\n",
    "\n",
    "# Add annotations\n",
    "ax.text(\n",
    "    x[0] - 3 * width / 4,\n",
    "    bottom_srs_rf[0],\n",
    "    \"RF\\nSRS\",  # Line break annotation\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\",\n",
    "    fontsize=9,\n",
    ")\n",
    "ax.text(\n",
    "    x[0] - width / 4,\n",
    "    bottom_fscs_rf[0] - 0.005,\n",
    "    \"RF\\nFSCS\",  # Line break annotation\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\",\n",
    "    fontsize=9,\n",
    ")\n",
    "ax.text(\n",
    "    x[0] + width / 4,\n",
    "    bottom_srs_lgb[0] + 0.01,\n",
    "    \"LGB\\nSRS\",  # Line break annotation\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\",\n",
    "    fontsize=9,\n",
    ")\n",
    "ax.text(\n",
    "    x[0] + 3 * width / 4,\n",
    "    bottom_fscs_lgb[0] - 0.01,\n",
    "    \"LGB\\nFSCS\",  # Line break annotation\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\",\n",
    "    fontsize=9,\n",
    ")\n",
    "\n",
    "# Beautify the chart\n",
    "ax.set_xlabel(\"Features\", fontsize=14)\n",
    "ax.set_ylabel(\"Normalization Mean |SHAP Value|\", fontsize=14)\n",
    "ax.set_title(\"SHAP Feature Importance for RF/LGB + SRS/FSCS\", fontsize=18)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(feature_names, rotation=45, ha=\"right\", fontsize=14)\n",
    "ax.legend(title=\"Classes\", fontsize=14, title_fontsize=16)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.grid(True, which=\"major\", axis=\"y\", linestyle=\"-\", linewidth=0.6)\n",
    "plt.grid(True, which=\"minor\", axis=\"y\", linestyle=\"--\", linewidth=0.25)\n",
    "plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.05))\n",
    "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shapclass.jpg\", format=\"jpg\", dpi=800, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c4bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Calculate the mean absolute SHAP value for each feature in each class\n",
    "shap_values_srs_rf_abs_mean = np.mean(np.abs(shap_values_srs_rf), axis=0)\n",
    "shap_values_fscs_rf_abs_mean = np.mean(np.abs(shap_values_fscs_rf), axis=0)\n",
    "shap_values_srs_lgb_abs_mean = np.mean(np.abs(shap_values_srs_lgb), axis=0)\n",
    "shap_values_fscs_lgb_abs_mean = np.mean(np.abs(shap_values_fscs_lgb), axis=0)\n",
    "\n",
    "# Normalize\n",
    "shap_values_srs_rf_abs_mean /= shap_values_srs_rf_abs_mean.sum()\n",
    "shap_values_fscs_rf_abs_mean /= shap_values_fscs_rf_abs_mean.sum()\n",
    "shap_values_srs_lgb_abs_mean /= shap_values_srs_lgb_abs_mean.sum()\n",
    "shap_values_fscs_lgb_abs_mean /= shap_values_fscs_lgb_abs_mean.sum()\n",
    "\n",
    "# Class and feature names\n",
    "class_names = [\n",
    "    \"Aspen\",\n",
    "    \"Cottonwood Willow\",\n",
    "    \"Douglas Fir\",\n",
    "    \"Krummholz\",\n",
    "    \"Lodgepole Pine\",\n",
    "    \"Ponderosa Pine\",\n",
    "    \"Spruce Fir\",\n",
    "]\n",
    "feature_names = [\n",
    "    \"Elevation\",\n",
    "    \"Aspect\",\n",
    "    \"Slope\",\n",
    "    \"Horizontal Distance to Hydrology\",\n",
    "    \"Vertical Distance to Hydrology\",\n",
    "    \"Horizontal Distance to Roadways\",\n",
    "    \"Hillshade 9am\",\n",
    "    \"Hillshade Noon\",\n",
    "    \"Hillshade 3pm\",\n",
    "    \"Horizontal Distance to Fire Points\",\n",
    "]\n",
    "\n",
    "# X axis positions and bar width\n",
    "num_features = len(feature_names)\n",
    "x = np.arange(num_features)\n",
    "width = 0.5\n",
    "\n",
    "# Create 2x2 subplots, share Y axis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12), sharey=True)\n",
    "axes = axes.flatten()\n",
    "methods = [\n",
    "    (shap_values_srs_rf_abs_mean, \"(a) RF + SRS\"),\n",
    "    (shap_values_fscs_rf_abs_mean, \"(b) RF + FSCS\"),\n",
    "    (shap_values_srs_lgb_abs_mean, \"(c) LGB + SRS\"),\n",
    "    (shap_values_fscs_lgb_abs_mean, \"(d) LGB + FSCS\"),\n",
    "]\n",
    "\n",
    "for idx, (shap_vals, title) in enumerate(methods):\n",
    "    ax = axes[idx]\n",
    "    bottom = np.zeros(num_features)\n",
    "    for i, cls in enumerate(class_names):\n",
    "        values = shap_vals[:, i]\n",
    "        ax.bar(\n",
    "            x,\n",
    "            values,\n",
    "            width,\n",
    "            bottom=bottom,\n",
    "            label=cls,\n",
    "        )\n",
    "        bottom += values\n",
    "    ax.set_title(title, fontsize=15)\n",
    "    ax.grid(True, which=\"major\", axis=\"y\", linestyle=\"-\", linewidth=0.6)\n",
    "    ax.grid(True, which=\"minor\", axis=\"y\", linestyle=\"--\", linewidth=0.25)\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.05))\n",
    "    ax.yaxis.set_minor_locator(plt.MultipleLocator(0.01))\n",
    "    ax.tick_params(axis=\"y\", labelsize=13)\n",
    "\n",
    "    # Only show X axis ticks and labels on the second row\n",
    "    if idx // 2 == 1:\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(feature_names, rotation=45, ha=\"right\", fontsize=13)\n",
    "        ax.set_xlabel(\"Features\", fontsize=13)\n",
    "    else:\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "    # Only show Y axis label on the first column\n",
    "    if idx % 2 == 0:\n",
    "        ax.set_ylabel(\"Normalized Mean |SHAP Value|\", fontsize=13)\n",
    "\n",
    "# Overall beautification\n",
    "fig.suptitle(\n",
    "    \"SHAP Feature Importance by Model + Sampling Method and Class\", fontsize=19\n",
    ")\n",
    "# Shared legend\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"Classes\",\n",
    "    bbox_to_anchor=(0.98, 0.92),\n",
    "    title_fontsize=14,\n",
    "    fontsize=12,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(\n",
    "    wspace=0.02, hspace=0.1\n",
    ")  # Adjust horizontal and vertical spacing between subplots\n",
    "plt.savefig(\"shap_class_fourpanels.png\", dpi=1000, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SHAP beeswarm plots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "# Calculate the mean SHAP value for each feature in each class\n",
    "shap_values_srs_rf_mean = np.mean(shap_values_srs_rf, axis=0)\n",
    "shap_values_fscs_rf_mean = np.mean(shap_values_fscs_rf, axis=0)\n",
    "shap_values_srs_lgb_mean = np.mean(shap_values_srs_lgb, axis=0)\n",
    "shap_values_fscs_lgb_mean = np.mean(shap_values_fscs_lgb, axis=0)\n",
    "\n",
    "# Class names\n",
    "class_names = [\n",
    "    \"Aspen\",\n",
    "    \"Cottonwood Willow\",\n",
    "    \"Douglas Fir\",\n",
    "    \"Krummholz\",\n",
    "    \"Lodgepole Pine\",\n",
    "    \"Ponderosa Pine\",\n",
    "    \"Spruce Fir\",\n",
    "]\n",
    "\n",
    "# Manually specify x-axis labels\n",
    "feature_names = [\n",
    "    \"Elevation\",\n",
    "    \"Aspect\",\n",
    "    \"Slope\",\n",
    "    \"Horizontal Distance\\nto Hydrology\",\n",
    "    \"Vertical Distance\\nto Hydrology\",\n",
    "    \"Horizontal Distance\\nto Roadways\",\n",
    "    \"Hillshade 9 am\",\n",
    "    \"Hillshade Noon\",\n",
    "    \"Hillshade 3 pm\",\n",
    "    \"Horizontal Distance\\nto Fire Points\",\n",
    "]\n",
    "\n",
    "# Set global font size\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "\n",
    "# Generate beeswarm plots for each class and save as image files\n",
    "def plot_beeswarm(shap_values, X, class_name, method_name, filename):\n",
    "    shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)\n",
    "    plt.title(f\"SHAP Values for {class_name} ({method_name})\", fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"SHAP Value (Impact on Model Output)\", fontsize=17)\n",
    "    plt.savefig(filename, dpi=800)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Generate beeswarm plots for SRS and FSCS and save as image files\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plot_beeswarm(\n",
    "        shap_values_srs_rf[:, :, i],\n",
    "        X_train_srs_rf,\n",
    "        class_name,\n",
    "        \"RF+SRS\",\n",
    "        f\"shap_srs_rf_{i}.png\",\n",
    "    )\n",
    "    plot_beeswarm(\n",
    "        shap_values_fscs_rf[:, :, i],\n",
    "        X_train_fscs_rf,\n",
    "        class_name,\n",
    "        \"RF+FSCS\",\n",
    "        f\"shap_fscs_rf_{i}.png\",\n",
    "    )\n",
    "    plot_beeswarm(\n",
    "        shap_values_srs_lgb[:, :, i],\n",
    "        X_train_srs_lgb,\n",
    "        class_name,\n",
    "        \"LGB+SRS\",\n",
    "        f\"shap_srs_lgb_{i}.png\",\n",
    "    )\n",
    "    plot_beeswarm(\n",
    "        shap_values_fscs_lgb[:, :, i],\n",
    "        X_train_fscs_lgb,\n",
    "        class_name,\n",
    "        \"LGB+FSCS\",\n",
    "        f\"shap_fscs_lgb_{i}.png\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the combined image for RF+SRS\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(10, 16))\n",
    "for i, class_name in enumerate(class_names):\n",
    "    row, col = divmod(i, 2)\n",
    "    axes[row, col].imshow(plt.imread(f\"shap_srs_rf_{i}.png\"))\n",
    "    axes[row, col].axis(\"off\")\n",
    "# Hide the last empty axis if the number of classes is odd\n",
    "if len(class_names) % 2 != 0:\n",
    "    axes[-1, -1].axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.savefig(\"combined_beeswarm_rf_srs.png\", dpi=800)\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create and save the combined image for RF+FSCS\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(10, 16))\n",
    "for i, class_name in enumerate(class_names):\n",
    "    row, col = divmod(i, 2)\n",
    "    axes[row, col].imshow(plt.imread(f\"shap_fscs_rf_{i}.png\"))\n",
    "    axes[row, col].axis(\"off\")\n",
    "# Hide the last empty axis if the number of classes is odd\n",
    "if len(class_names) % 2 != 0:\n",
    "    axes[-1, -1].axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.savefig(\"combined_beeswarm_rf_fscs.png\", dpi=800)\n",
    "plt.show()\n",
    "\n",
    "# Create and save the combined image for LGB+SRS\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(10, 16))\n",
    "for i, class_name in enumerate(class_names):\n",
    "    row, col = divmod(i, 2)\n",
    "    axes[row, col].imshow(plt.imread(f\"shap_srs_lgb_{i}.png\"))\n",
    "    axes[row, col].axis(\"off\")\n",
    "# Hide the last empty axis if the number of classes is odd\n",
    "if len(class_names) % 2 != 0:\n",
    "    axes[-1, -1].axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.savefig(\"combined_beeswarm_lgb_srs.png\", dpi=800)\n",
    "plt.show()\n",
    "\n",
    "# Create and save the combined image for LGB+FSCS\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(10, 16))\n",
    "for i, class_name in enumerate(class_names):\n",
    "    row, col = divmod(i, 2)\n",
    "    axes[row, col].imshow(plt.imread(f\"shap_fscs_lgb_{i}.png\"))\n",
    "    axes[row, col].axis(\"off\")\n",
    "# Hide the last empty axis if the number of classes is odd\n",
    "if len(class_names) % 2 != 0:\n",
    "    axes[-1, -1].axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.savefig(\"combined_beeswarm_lgb_fscs.png\", dpi=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a DataFrame for all data\n",
    "df_overall = pd.DataFrame({\"Class\": class_df[target], \"Dataset\": \"Overall\"})\n",
    "df_srs_rf = pd.DataFrame({\"Class\": y_train_srs_rf, \"Dataset\": \"RF+SRS Train\"})\n",
    "df_fscs_rf = pd.DataFrame({\"Class\": y_train_fscs_rf, \"Dataset\": \"RF+FSCS Train\"})\n",
    "df_srs_lgb = pd.DataFrame({\"Class\": y_train_srs_lgb, \"Dataset\": \"LGB+SRS Train\"})\n",
    "df_fscs_lgb = pd.DataFrame({\"Class\": y_train_fscs_lgb, \"Dataset\": \"LGB+FSCS Train\"})\n",
    "\n",
    "# Combine all data\n",
    "df_combined = pd.concat([df_overall, df_srs_rf, df_fscs_rf, df_srs_lgb, df_fscs_lgb])\n",
    "\n",
    "# Convert the Class column to an ordered categorical type\n",
    "class_names = [\n",
    "    \"Aspen\",\n",
    "    \"Cottonwood_Willow\",\n",
    "    \"Douglas_fir\",\n",
    "    \"Krummholz\",\n",
    "    \"Lodgepole_Pine\",\n",
    "    \"Ponderosa_Pine\",\n",
    "    \"Spruce_Fir\",\n",
    "]\n",
    "\n",
    "class_labels = [\n",
    "    \"Aspen\",\n",
    "    \"Cottonwood Willow\",\n",
    "    \"Douglas Fir\",\n",
    "    \"Krummholz\",\n",
    "    \"Lodgepole Pine\",\n",
    "    \"Ponderosa Pine\",\n",
    "    \"Spruce Fir\",\n",
    "]\n",
    "df_combined[\"Class\"] = pd.Categorical(\n",
    "    df_combined[\"Class\"], categories=class_names, ordered=True\n",
    ")\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a histplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(\n",
    "    data=df_combined,\n",
    "    x=\"Class\",\n",
    "    hue=\"Dataset\",\n",
    "    stat=\"density\",\n",
    "    common_norm=False,\n",
    "    element=\"bars\",\n",
    "    multiple=\"dodge\",\n",
    "    shrink=0.8,\n",
    ")\n",
    "\n",
    "# Set x-axis category order\n",
    "plt.xticks(ticks=range(len(class_names)), labels=class_labels)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Class Distribution by Sampling Method (Density)\", fontsize=15)\n",
    "plt.xlabel(\"Classes\", fontsize=14)\n",
    "plt.ylabel(\"Density\", fontsize=14)\n",
    "plt.savefig(\"Class_Distribution_compare.png\", dpi=800)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e02a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariate feature distributions\n",
    "# Please note that this image was saved manually. Using plt.savefig caused the figure to become distorted.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define covariate features and target variable\n",
    "features = [\n",
    "    \"elevation\",\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"horizontal_distance_to_hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\",\n",
    "    \"Hillshade_Noon\",\n",
    "    \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\",\n",
    "]\n",
    "\n",
    "feature_names = [\n",
    "    \"Elevation\",\n",
    "    \"Aspect\",\n",
    "    \"Slope\",\n",
    "    \"Horizontal Distance\\nto Hydrology\",\n",
    "    \"Vertical Distance\\nto Hydrology\",\n",
    "    \"Horizontal Distance\\nto Roadways\",\n",
    "    \"Hillshade 9 am\",\n",
    "    \"Hillshade Noon\",\n",
    "    \"Hillshade 3 pm\",\n",
    "    \"Horizontal Distance\\nto Fire Points\",\n",
    "]\n",
    "\n",
    "# Create a combined DataFrame containing all datasets\n",
    "df_combined = pd.concat(\n",
    "    [\n",
    "        class_df.assign(Dataset=\"Overall\"),\n",
    "        X_train_srs_rf.assign(Dataset=\"RF+SRS Train\"),\n",
    "        X_train_fscs_rf.assign(Dataset=\"RF+FSCS Train\"),\n",
    "        X_train_srs_lgb.assign(Dataset=\"LGB+SRS Train\"),\n",
    "        X_train_fscs_lgb.assign(Dataset=\"LGB+FSCS Train\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define different linestyles and linewidths for each dataset\n",
    "linestyles = {\n",
    "    \"Overall\": \"-\",\n",
    "    \"RF+SRS Train\": \"--\",\n",
    "    \"RF+FSCS Train\": \"-.\",\n",
    "    \"LGB+SRS Train\": \"--\",\n",
    "    \"LGB+FSCS Train\": \"-.\",\n",
    "}\n",
    "linewidths = {\n",
    "    \"Overall\": 3.5,\n",
    "    \"RF+SRS Train\": 2.5,\n",
    "    \"RF+FSCS Train\": 2.5,\n",
    "    \"LGB+SRS Train\": 2.5,\n",
    "    \"LGB+FSCS Train\": 2.5,\n",
    "}\n",
    "\n",
    "# Define colors for each dataset\n",
    "colors = {\n",
    "    \"Overall\": \"black\",\n",
    "    \"RF+SRS Train\": \"red\",\n",
    "    \"RF+FSCS Train\": \"blue\",\n",
    "    \"LGB+SRS Train\": \"darkred\",\n",
    "    \"LGB+FSCS Train\": \"darkblue\",\n",
    "}\n",
    "\n",
    "# Create a 4x3 subplot layout, one subplot for each feature\n",
    "fig, axes = plt.subplots(4, 3, figsize=(25, 30))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "for i, (feature, feature_name) in enumerate(zip(features, feature_names)):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    df_combined_data = df_combined[[feature, \"Dataset\"]]\n",
    "    for j, (dataset, linestyle) in enumerate(\n",
    "        zip(\n",
    "            [\n",
    "                \"Overall\",\n",
    "                \"RF+SRS Train\",\n",
    "                \"RF+FSCS Train\",\n",
    "                \"LGB+SRS Train\",\n",
    "                \"LGB+FSCS Train\",\n",
    "            ],\n",
    "            linestyles,\n",
    "        )\n",
    "    ):\n",
    "        g = sns.kdeplot(\n",
    "            data=df_combined_data[df_combined_data[\"Dataset\"] == dataset],\n",
    "            x=feature,\n",
    "            label=dataset,\n",
    "            ax=ax,\n",
    "            linestyle=linestyles[dataset],\n",
    "            linewidth=linewidths[dataset],\n",
    "            color=colors[dataset],  # Specify color\n",
    "        )\n",
    "    ax.set_title(feature_name, fontsize=22)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Density\", fontsize=20)\n",
    "    ax.set_yscale(\"log\")  # Set y-axis to log scale\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "    if i % 3 == 0:\n",
    "        ax.set_ylabel(\"Density\", fontsize=22)\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "# Remove the last two empty subplots\n",
    "axes[3, 1].axis(\"off\")\n",
    "axes[3, 2].axis(\"off\")\n",
    "# Place the legend outside the first subplot, to the upper left\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.43, 1.031),\n",
    "    fontsize=21,\n",
    "    title=\"Dataset\",\n",
    "    title_fontsize=21,\n",
    "    ncol=5,\n",
    ")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "# plt.savefig(\"Class_Feature_Distribution_compare.png\", dpi=800)\n",
    "# The code is commented out due to savefig deformation.\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2238eb9c",
   "metadata": {},
   "source": [
    "The following is the Ksat drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d99050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All training and testing sets generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Read the original dataset\n",
    "class_df = pd.read_csv(\n",
    "    \"sampleddata/USKSAT_OpenRefined_cleaned.csv\",\n",
    "    index_col=0,  # Use the first column as index\n",
    ")\n",
    "\n",
    "# Convert the target variable to natural logarithm\n",
    "class_df[\"Ksat_cmhr\"] = np.log(class_df[\"Ksat_cmhr\"])\n",
    "\n",
    "# Define sample sizes and sampling methods\n",
    "sample_levels = [1000, 5000, 10000]\n",
    "\n",
    "sampling_methods = [\n",
    "    \"BalancedSampling_sampled_data\",\n",
    "    \"clhs_sampled_data\",\n",
    "    \"FCMtp2sampled_data\",\n",
    "    \"FSCS_sampled_data\",\n",
    "]\n",
    "\n",
    "# Define covariates and target variable\n",
    "covariates = [\n",
    "    \"Db\",\n",
    "    \"Clay\",\n",
    "    \"VFS\",\n",
    "    \"MS\",\n",
    "    \"OC\",\n",
    "    \"Silt\",\n",
    "    \"COS\",\n",
    "    \"FS\",\n",
    "    \"Depth.cm_Top\",\n",
    "    \"VCOS\",\n",
    "]\n",
    "covariates_FSCS = [\n",
    "    \"Db\",\n",
    "    \"Clay\",\n",
    "    \"VFS\",\n",
    "    \"MS\",\n",
    "    \"OC\",\n",
    "    \"Silt\",\n",
    "    \"COS\",\n",
    "    \"FS\",\n",
    "    \"Depth_cm_Top\",\n",
    "    \"VCOS\",\n",
    "]\n",
    "target = \"Ksat_cmhr\"\n",
    "\n",
    "# Store train and test sets\n",
    "train_test_data = {}\n",
    "\n",
    "# Loop through each sample size and sampling method\n",
    "for level in sample_levels:\n",
    "    for t in range(1, 21):\n",
    "        for method in sampling_methods:\n",
    "            sample_file = (\n",
    "                f\"sampleddata/combined_samples_Ksat/{method}_{level}_set_{t}.csv\"\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(sample_file):\n",
    "                print(f\"File {sample_file} does not exist. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            sample_data = pd.read_csv(\n",
    "                sample_file, index_col=0\n",
    "            )  # Use the first column as index\n",
    "            sample_data[target] = np.log(\n",
    "                sample_data[target]\n",
    "            )  # Convert the target variable to natural logarithm\n",
    "            if method == \"FSCS_sampled_data\":\n",
    "                X_train = sample_data[covariates_FSCS]\n",
    "                X_train = X_train.rename(columns={\"Depth_cm_Top\": \"DT\"})\n",
    "            else:\n",
    "                X_train = sample_data[covariates]\n",
    "                X_train = X_train.rename(columns={\"Depth.cm_Top\": \"DT\"})\n",
    "\n",
    "            y_train = sample_data[target]\n",
    "            y_train.name = \"Ks\"\n",
    "            train_indices = sample_data.index\n",
    "\n",
    "            # Generate the corresponding test set\n",
    "            test_data = class_df.drop(train_indices)\n",
    "            X_test = test_data[covariates]\n",
    "            X_test = X_test.rename(columns={\"Depth.cm_Top\": \"DT\"})\n",
    "            y_test = test_data[target]\n",
    "            y_test.name = \"Ks\"\n",
    "\n",
    "            # Store train and test sets\n",
    "            train_test_data[f\"{method}_{level}_set_{t}\"] = (\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_test,\n",
    "                y_test,\n",
    "            )\n",
    "\n",
    "# Generate SRS sample subsets\n",
    "for level in sample_levels:\n",
    "    for t in range(1, 21):\n",
    "        # Simple random sampling\n",
    "        srs_sample = resample(class_df, n_samples=level, random_state=42 + t)\n",
    "        X_train = srs_sample[covariates]\n",
    "        y_train = srs_sample[target]\n",
    "        train_indices = srs_sample.index\n",
    "\n",
    "        # Generate the corresponding test set\n",
    "        test_data = class_df.drop(train_indices)\n",
    "        X_test = test_data[covariates]\n",
    "        y_test = test_data[target]\n",
    "\n",
    "        X_train = X_train.rename(columns={\"Depth.cm_Top\": \"DT\"})\n",
    "        X_test = X_test.rename(columns={\"Depth.cm_Top\": \"DT\"})\n",
    "        y_train.name = \"Ks\"\n",
    "        y_test.name = \"Ks\"\n",
    "\n",
    "        # Store train and test sets\n",
    "        train_test_data[f\"SRS_sampled_df_{level}_set_{t}\"] = (\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "        )\n",
    "\n",
    "print(\"All training and testing sets generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feed66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved DataFrame files\n",
    "results_df_rfksat = pd.read_csv(\"rfKsatresults_all.csv\")\n",
    "results_df_lgbksat = pd.read_csv(\"lgbKsatresults_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fc28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest comparison: SRS vs FSCS at 10000 sample level for regression (continuous variable)\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import fasttreeshap\n",
    "\n",
    "# Create a new variable to store extracted sampling method and dataset level\n",
    "results_with_methods = results_df_rfksat.copy()\n",
    "\n",
    "results_with_methods[\"sampling_method\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[0]\n",
    ")\n",
    "results_with_methods[\"dataset_level\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[-3]\n",
    ")\n",
    "\n",
    "results_with_methods[\"set_number\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[-1]\n",
    ")\n",
    "\n",
    "# Calculate the average of two metrics (R2 and adjusted RMSLE)\n",
    "results_with_methods[\"adjusted_rmsle\"] = 1 - results_with_methods[\"rmsle\"]\n",
    "results_with_methods[\"average_metric\"] = results_with_methods[\n",
    "    [\"r2\", \"adjusted_rmsle\"]\n",
    "].mean(axis=1)\n",
    "\n",
    "# Find the median-performing result for SRS at 10000 level\n",
    "sorted_results_srs = results_with_methods[\n",
    "    (results_with_methods[\"sampling_method\"] == \"SRS\")\n",
    "    & (results_with_methods[\"dataset_level\"] == \"10000\")\n",
    "].sort_values(by=\"average_metric\", ascending=False)\n",
    "\n",
    "# Find the median-performing result for FSCS at 10000 level\n",
    "sorted_results_fscs = results_with_methods[\n",
    "    (results_with_methods[\"sampling_method\"] == \"FSCS\")\n",
    "    & (results_with_methods[\"dataset_level\"] == \"10000\")\n",
    "].sort_values(by=\"average_metric\", ascending=False)\n",
    "\n",
    "# Get the median value for each method\n",
    "median_metric_srs = sorted_results_srs[\"average_metric\"].median()\n",
    "median_metric = sorted_results_fscs[\"average_metric\"].median()\n",
    "\n",
    "# Find the result closest to the median for each method\n",
    "sorted_results_srs[\"metric_diff\"] = (\n",
    "    sorted_results_srs[\"average_metric\"] - median_metric\n",
    ").abs()\n",
    "middle_result_srs = sorted_results_srs.loc[sorted_results_srs[\"metric_diff\"].idxmin()]\n",
    "\n",
    "sorted_results_fscs[\"metric_diff\"] = (\n",
    "    sorted_results_fscs[\"average_metric\"] - median_metric\n",
    ").abs()\n",
    "middle_result_fscs = sorted_results_fscs.loc[\n",
    "    sorted_results_fscs[\"metric_diff\"].idxmin()\n",
    "]\n",
    "\n",
    "# Get the set number for each selected result\n",
    "set_number_srs = middle_result_srs[\"set_number\"]\n",
    "set_number_fscs = middle_result_fscs[\"set_number\"]\n",
    "\n",
    "# Build file path for SRS SHAP values\n",
    "file_path_srs = (\n",
    "    f\"codepart\\\\RFKsatpkl\\\\SRS_sampled_df_10000_set_{set_number_srs}_shap_values_v2.pkl\"\n",
    ")\n",
    "\n",
    "# Load SRS SHAP values from pkl file\n",
    "shap_values_srs_rf = joblib.load(file_path_srs)\n",
    "\n",
    "# Build file path for FSCS SHAP values\n",
    "file_path_fscs = f\"codepart\\\\RFKsatpkl\\\\FSCS_sampled_data_10000_set_{set_number_fscs}_shap_values_v2.pkl\"\n",
    "\n",
    "# Load FSCS SHAP values from pkl file\n",
    "shap_values_fscs_rf = joblib.load(file_path_fscs)\n",
    "\n",
    "# Build file path for SRS SHAP interaction values\n",
    "file_path_srs_inter = f\"codepart\\\\RFKsatpkl\\\\SRS_sampled_df_10000_set_{set_number_srs}_shap_interaction_values_v1.pkl\"\n",
    "\n",
    "# Load SRS SHAP interaction values from pkl file\n",
    "shap_values_srs_rf_inter = joblib.load(file_path_srs_inter)\n",
    "\n",
    "# Build file path for FSCS SHAP interaction values\n",
    "file_path_fscs_inter = f\"codepart\\\\RFKsatpkl\\\\FSCS_sampled_data_10000_set_{set_number_fscs}_shap_interaction_values_v1.pkl\"\n",
    "\n",
    "# Load FSCS SHAP interaction values from pkl file\n",
    "shap_values_fscs_rf_inter = joblib.load(file_path_fscs_inter)\n",
    "\n",
    "# Build dataset keys\n",
    "dataset_srs = f\"SRS_sampled_df_10000_set_{set_number_srs}\"\n",
    "dataset_fscs = f\"FSCS_sampled_data_10000_set_{set_number_srs}\"\n",
    "\n",
    "# Retrieve corresponding datasets\n",
    "X_train_srs_rf, y_train_srs_rf, X_test_srs_rf, y_test_srs_rf = train_test_data[\n",
    "    dataset_srs\n",
    "]\n",
    "X_train_fscs_rf, y_train_fscs_rf, X_test_fscs_rf, y_test_fscs_rf = train_test_data[\n",
    "    dataset_fscs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RF beeswarm plots\n",
    "\n",
    "import shap\n",
    "\n",
    "# Manually specify x-axis labels\n",
    "feature_names = [\n",
    "    \"Db\",\n",
    "    \"Clay\",\n",
    "    \"VFS\",\n",
    "    \"MS\",\n",
    "    \"OC\",\n",
    "    \"Silt\",\n",
    "    \"COS\",\n",
    "    \"FS\",\n",
    "    \"Depth_cm_Top\",\n",
    "    \"VCOS\",\n",
    "]\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values_srs_rf, X_train_srs_rf, feature_names=feature_names, show=False\n",
    ")\n",
    "plt.title(f\"SHAP Values for RF + SRS\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"beeswarm_rfksat_srs.png\", dpi=800)\n",
    "plt.show()\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values_fscs_rf, X_train_fscs_rf, feature_names=feature_names, show=False\n",
    ")\n",
    "plt.title(f\"SHAP Values for RF + FSCS\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"beeswarm_rfksat_fscs.png\", dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM comparison: SRS vs FSCS at 10000 sample level for regression (continuous variable)\n",
    "import joblib\n",
    "import fasttreeshap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a new variable to store extracted sampling method and dataset level\n",
    "results_with_methods = results_df_lgbksat.copy()\n",
    "\n",
    "results_with_methods[\"sampling_method\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[0]\n",
    ")\n",
    "results_with_methods[\"dataset_level\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[-3]\n",
    ")\n",
    "\n",
    "results_with_methods[\"set_number\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[-1]\n",
    ")\n",
    "\n",
    "# Calculate the average of two metrics (R2 and RMSLE)\n",
    "results_with_methods[\"average_metric\"] = results_with_methods[[\"r2\", \"rmsle\"]].mean(\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Find the median-performing result for SRS at 10000 level\n",
    "sorted_results_srs = results_with_methods[\n",
    "    (results_with_methods[\"sampling_method\"] == \"SRS\")\n",
    "    & (results_with_methods[\"dataset_level\"] == \"10000\")\n",
    "].sort_values(by=\"average_metric\", ascending=False)\n",
    "\n",
    "# Find the median-performing result for FSCS at 10000 level\n",
    "sorted_results_fscs = results_with_methods[\n",
    "    (results_with_methods[\"sampling_method\"] == \"FSCS\")\n",
    "    & (results_with_methods[\"dataset_level\"] == \"10000\")\n",
    "].sort_values(by=\"average_metric\", ascending=False)\n",
    "\n",
    "# Get the median value for each method\n",
    "median_metric_srs = sorted_results_srs[\"average_metric\"].median()\n",
    "median_metric = sorted_results_fscs[\"average_metric\"].median()\n",
    "\n",
    "# Find the result closest to the median for each method\n",
    "sorted_results_srs[\"metric_diff\"] = (\n",
    "    sorted_results_srs[\"average_metric\"] - median_metric\n",
    ").abs()\n",
    "middle_result_srs = sorted_results_srs.loc[sorted_results_srs[\"metric_diff\"].idxmin()]\n",
    "\n",
    "sorted_results_fscs[\"metric_diff\"] = (\n",
    "    sorted_results_fscs[\"average_metric\"] - median_metric\n",
    ").abs()\n",
    "middle_result_fscs = sorted_results_fscs.loc[\n",
    "    sorted_results_fscs[\"metric_diff\"].idxmin()\n",
    "]\n",
    "\n",
    "# Get the set number for each selected result\n",
    "set_number_srs = middle_result_srs[\"set_number\"]\n",
    "set_number_fscs = middle_result_fscs[\"set_number\"]\n",
    "\n",
    "# Build file paths\n",
    "file_path_srs = (\n",
    "    f\"codepart\\\\LGBKsatpkl\\\\SRS_sampled_df_10000_set_{set_number_srs}_model.pkl\"\n",
    ")\n",
    "clf_srs = joblib.load(file_path_srs)\n",
    "file_path_fscs = (\n",
    "    f\"codepart\\\\LGBKsatpkl\\\\FSCS_sampled_data_10000_set_{set_number_fscs}_model.pkl\"\n",
    ")\n",
    "clf_fscs = joblib.load(file_path_fscs)\n",
    "\n",
    "# Build dataset keys\n",
    "dataset_srs = f\"SRS_sampled_df_10000_set_{set_number_srs}\"\n",
    "dataset_fscs = f\"FSCS_sampled_data_10000_set_{set_number_fscs}\"\n",
    "\n",
    "# Retrieve corresponding datasets\n",
    "(\n",
    "    X_train_srs_lgb,\n",
    "    y_train_srs_lgb,\n",
    "    X_test_srs_lgb,\n",
    "    y_test_srs_lgb,\n",
    ") = train_test_data[dataset_srs]\n",
    "\n",
    "X_train_fscs_lgb, y_train_fscs_lgb, X_test_fscs_lgb, y_test_fscs_lgb = train_test_data[\n",
    "    dataset_fscs\n",
    "]\n",
    "\n",
    "# Calculate SHAP values and SHAP interaction values for SRS\n",
    "shap_explainer_srs = fasttreeshap.TreeExplainer(\n",
    "    clf_srs, algorithm=\"v0\", n_jobs=-1, shortcut=True\n",
    ")\n",
    "shap_values_srs_lgb = shap_explainer_srs(X_train_srs_lgb).values\n",
    "shap_values_srs_lgb_inter = shap_explainer_srs(\n",
    "    X_train_srs_lgb, interactions=True\n",
    ").values\n",
    "\n",
    "# Calculate SHAP values and SHAP interaction values for FSCS\n",
    "shap_explainer_srs = fasttreeshap.TreeExplainer(\n",
    "    clf_srs, algorithm=\"v0\", n_jobs=-1, shortcut=True\n",
    ")\n",
    "shap_values_srs_lgb = shap_explainer_srs(X_train_srs_lgb).values\n",
    "shap_values_srs_lgb_inter = shap_explainer_srs(\n",
    "    X_train_srs_lgb, interactions=True\n",
    ").values\n",
    "\n",
    "shap_explainer_fscs = fasttreeshap.TreeExplainer(\n",
    "    clf_fscs, algorithm=\"v0\", n_jobs=-1, shortcut=True\n",
    ")\n",
    "shap_values_fscs_lgb = shap_explainer_fscs(X_train_fscs_lgb).values\n",
    "shap_values_fscs_lgb_inter = shap_explainer_fscs(\n",
    "    X_train_fscs_lgb, interactions=True\n",
    ").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LightGBM beeswarm plots\n",
    "\n",
    "import shap\n",
    "\n",
    "# Manually specify x-axis labels\n",
    "feature_names = [\n",
    "    \"Db\",\n",
    "    \"Clay\",\n",
    "    \"VFS\",\n",
    "    \"MS\",\n",
    "    \"OC\",\n",
    "    \"Silt\",\n",
    "    \"COS\",\n",
    "    \"FS\",\n",
    "    \"DT\",\n",
    "    \"VCOS\",\n",
    "]\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values_srs_rf, X_train_srs_rf, feature_names=feature_names, show=False\n",
    ")\n",
    "plt.title(f\"SHAP Values for RF + SRS\", fontsize=20)\n",
    "plt.xlabel(\"SHAP Value (Impact on Model Output)\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"beeswarm_rfksat_srs.png\", dpi=800)\n",
    "plt.close()\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values_fscs_rf, X_train_fscs_rf, feature_names=feature_names, show=False\n",
    ")\n",
    "plt.title(f\"SHAP Values for RF + FSCS\", fontsize=20)\n",
    "plt.xlabel(\"SHAP Value (Impact on Model Output)\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"beeswarm_rfksat_fscs.png\", dpi=800)\n",
    "plt.close()\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values_srs_lgb, X_train_srs_lgb, feature_names=feature_names, show=False\n",
    ")\n",
    "plt.title(f\"SHAP Values for LGB + SRS\", fontsize=20)\n",
    "plt.xlabel(\"SHAP Value (Impact on Model Output)\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"beeswarm_lgbksat_srs.png\", dpi=800)\n",
    "plt.close()\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values_fscs_lgb, X_train_fscs_lgb, feature_names=feature_names, show=False\n",
    ")\n",
    "plt.title(f\"SHAP Values for LGB + FSCS\", fontsize=20)\n",
    "plt.xlabel(\"SHAP Value (Impact on Model Output)\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"beeswarm_lgbksat_fscs.png\", dpi=800)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x2 layout, each subplot for one feature importance plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "\n",
    "def plot_feature_importance(shap_values, X, feature_names, ax, title):\n",
    "    shap_values_mean = np.abs(shap_values).mean(axis=0)\n",
    "    sorted_idx = np.argsort(shap_values_mean)\n",
    "    sorted_feature_names = np.array(feature_names)[sorted_idx]\n",
    "    sorted_shap_values_mean = shap_values_mean[sorted_idx]\n",
    "    ax.barh(sorted_feature_names, sorted_shap_values_mean)\n",
    "    ax.set_title(title, fontsize=22)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "    ax.set_xlabel(\"Mean |SHAP Value|\", fontsize=18)\n",
    "\n",
    "\n",
    "# Plot feature importance for RF + SRS\n",
    "plot_feature_importance(\n",
    "    shap_values_srs_rf,\n",
    "    X_train_srs_rf,\n",
    "    feature_names,\n",
    "    axes[0, 0],\n",
    "    \"Feature Importance for RF + SRS\",\n",
    ")\n",
    "\n",
    "# Plot feature importance for RF + FSCS\n",
    "plot_feature_importance(\n",
    "    shap_values_fscs_rf,\n",
    "    X_train_fscs_rf,\n",
    "    feature_names,\n",
    "    axes[0, 1],\n",
    "    \"Feature Importance for RF + FSCS\",\n",
    ")\n",
    "\n",
    "# Plot feature importance for LGB + SRS\n",
    "plot_feature_importance(\n",
    "    shap_values_srs_lgb,\n",
    "    X_train_srs_lgb,\n",
    "    feature_names,\n",
    "    axes[1, 0],\n",
    "    \"Feature Importance for LGB + SRS\",\n",
    ")\n",
    "\n",
    "# Plot feature importance for LGB + FSCS\n",
    "plot_feature_importance(\n",
    "    shap_values_fscs_lgb,\n",
    "    X_train_fscs_lgb,\n",
    "    feature_names,\n",
    "    axes[1, 1],\n",
    "    \"Feature Importance for LGB + FSCS\",\n",
    ")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"feature_importance_all_methods.png\", dpi=800)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import anderson_ksamp\n",
    "from scipy.stats import cramervonmises_2samp\n",
    "\n",
    "variable_ori = \"Ksat_cmhr\"\n",
    "variable_new = \"Ks\"\n",
    "\n",
    "# Create a DataFrame containing all data\n",
    "df_overall = pd.DataFrame({variable_new: class_df[variable_ori], \"Dataset\": \"Overall\"})\n",
    "df_srs_rf = pd.DataFrame({variable_new: y_train_srs_rf, \"Dataset\": \"RF+SRS Train\"})\n",
    "df_fscs_rf = pd.DataFrame({variable_new: y_train_fscs_rf, \"Dataset\": \"RF+FSCS Train\"})\n",
    "df_srs_lgb = pd.DataFrame({variable_new: y_train_srs_lgb, \"Dataset\": \"LGB+SRS Train\"})\n",
    "df_fscs_lgb = pd.DataFrame(\n",
    "    {variable_new: y_train_fscs_lgb, \"Dataset\": \"LGB+FSCS Train\"}\n",
    ")\n",
    "\n",
    "# Concatenate all data\n",
    "df_combined = pd.concat([df_overall, df_srs_rf, df_fscs_rf, df_srs_lgb, df_fscs_lgb])\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a 5-row, 1-column layout\n",
    "fig, axes = plt.subplots(5, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "# Plot the density and histogram for each group\n",
    "for i, (dataset, ax) in enumerate(zip(df_combined[\"Dataset\"].unique(), axes)):\n",
    "    subset = df_combined[df_combined[\"Dataset\"] == dataset]\n",
    "    sns.histplot(\n",
    "        data=subset,\n",
    "        x=variable_new,\n",
    "        stat=\"density\",\n",
    "        fill=True,\n",
    "        bins=50,  # Set the number of histogram bins\n",
    "        ax=ax,\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        data=subset,\n",
    "        x=variable_new,\n",
    "        label=dataset,\n",
    "        bw_adjust=0.5,\n",
    "        fill=True,\n",
    "        log_scale=(False, True),\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(f\"{dataset} Distribution\")\n",
    "    if dataset == \"RF+FSCS Train\":\n",
    "        ax.set_ylabel(\"Density (Log Scale)\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "\n",
    "# Set x-axis label\n",
    "axes[-1].set_xlabel(\"ln(Ks)\")\n",
    "# Set x-axis tick interval to 1\n",
    "axes[-1].xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "\n",
    "# Set the same y-axis range for all subplots\n",
    "y_min, y_max = axes[0].get_ylim()\n",
    "for ax in axes:\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Ksat_Distribution_compare.png\", dpi=800)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f750ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define covariates and target variable\n",
    "features = [\n",
    "    \"Db\",\n",
    "    \"Clay\",\n",
    "    \"VFS\",\n",
    "    \"MS\",\n",
    "    \"OC\",\n",
    "    \"Silt\",\n",
    "    \"COS\",\n",
    "    \"FS\",\n",
    "    \"DT\",\n",
    "    \"VCOS\",\n",
    "]\n",
    "\n",
    "# Manually specify x-axis labels and units\n",
    "feature_names = [\n",
    "    \"Db (g/cm$^3$)\",\n",
    "    \"Clay (%)\",\n",
    "    \"VFS (%)\",\n",
    "    \"MS (%)\",\n",
    "    \"OC (%)\",\n",
    "    \"Silt (%)\",\n",
    "    \"COS (%)\",\n",
    "    \"FS (%)\",\n",
    "    \"DT (cm)\",\n",
    "    \"VCOS (%)\",\n",
    "]\n",
    "\n",
    "class_df.rename(columns={\"Depth.cm_Top\": \"DT\"}, inplace=True)\n",
    "# Create a DataFrame containing all data\n",
    "df_combined = pd.concat(\n",
    "    [\n",
    "        class_df.assign(Dataset=\"Overall\"),\n",
    "        X_train_srs_rf.assign(Dataset=\"RF+SRS Train\"),\n",
    "        X_train_fscs_rf.assign(Dataset=\"RF+FSCS Train\"),\n",
    "        X_train_srs_lgb.assign(Dataset=\"LGB+SRS Train\"),\n",
    "        X_train_fscs_lgb.assign(Dataset=\"LGB+FSCS Train\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define different linestyles and widths\n",
    "linestyles = {\n",
    "    \"Overall\": \"-\",\n",
    "    \"RF+SRS Train\": \"--\",\n",
    "    \"RF+FSCS Train\": \"-.\",\n",
    "    \"LGB+SRS Train\": \"--\",\n",
    "    \"LGB+FSCS Train\": \"-.\",\n",
    "}\n",
    "linewidths = {\n",
    "    \"Overall\": 3.5,\n",
    "    \"RF+SRS Train\": 2.5,\n",
    "    \"RF+FSCS Train\": 2.5,\n",
    "    \"LGB+SRS Train\": 2.5,\n",
    "    \"LGB+FSCS Train\": 2.5,\n",
    "}\n",
    "\n",
    "# Define colors\n",
    "colors = {\n",
    "    \"Overall\": \"black\",\n",
    "    \"RF+SRS Train\": \"red\",\n",
    "    \"RF+FSCS Train\": \"blue\",\n",
    "    \"LGB+SRS Train\": \"darkred\",\n",
    "    \"LGB+FSCS Train\": \"darkblue\",\n",
    "}\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Create a 4-row, 3-column layout, one subplot for each feature\n",
    "fig, axes = plt.subplots(4, 3, figsize=(25, 30))\n",
    "\n",
    "for i, (feature, feature_name) in enumerate(zip(features, feature_names)):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    df_combined_data = df_combined[[feature, \"Dataset\"]]\n",
    "    for j, (dataset, linestyle) in enumerate(\n",
    "        zip(\n",
    "            [\n",
    "                \"Overall\",\n",
    "                \"RF+SRS Train\",\n",
    "                \"RF+FSCS Train\",\n",
    "                \"LGB+SRS Train\",\n",
    "                \"LGB+FSCS Train\",\n",
    "            ],\n",
    "            linestyles,\n",
    "        )\n",
    "    ):\n",
    "        g = sns.kdeplot(\n",
    "            data=df_combined_data[df_combined_data[\"Dataset\"] == dataset],\n",
    "            x=feature,\n",
    "            label=dataset,\n",
    "            ax=ax,\n",
    "            linestyle=linestyles[dataset],\n",
    "            linewidth=linewidths[dataset],\n",
    "            color=colors[dataset],  # Specify color\n",
    "        )\n",
    "    ax.set_title(feature_name, fontsize=22)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Density\", fontsize=20)\n",
    "    ax.set_yscale(\"log\")  # Set y-axis to log scale\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "    if i % 3 == 0:\n",
    "        ax.set_ylabel(\"Density\", fontsize=22)\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "\n",
    "# Remove the last two empty subplots\n",
    "axes[3, 1].axis(\"off\")\n",
    "axes[3, 2].axis(\"off\")\n",
    "\n",
    "# Place the legend outside the upper left of the first subplot\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.43, 1.031),\n",
    "    fontsize=21,\n",
    "    title=\"Dataset\",\n",
    "    title_fontsize=21,\n",
    "    ncol=5,\n",
    ")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "plt.savefig(\"Ksat_Feature_Distribution_compare.png\", dpi=800)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the 'Clay' feature\n",
    "clay_idx = 1\n",
    "\n",
    "# Create a 3x3 subplot layout, one subplot for each feature pair\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
    "\n",
    "# Plot SHAP interaction dependence plots for each feature with 'Clay'\n",
    "for i, feature_name in enumerate(features):\n",
    "    if i >= 2:\n",
    "        i = i - 1\n",
    "        if feature_name != \"Clay\":\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            shap.dependence_plot(\n",
    "                (clay_idx, i + 1),\n",
    "                shap_values_srs_rf_inter,\n",
    "                X_train_srs_rf,\n",
    "                feature_names=feature_names,\n",
    "                ax=axes[row, col],\n",
    "                show=False,\n",
    "            )\n",
    "    else:\n",
    "        if feature_name != \"Clay\":\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            shap.dependence_plot(\n",
    "                (clay_idx, i),\n",
    "                shap_values_srs_rf_inter,\n",
    "                X_train_srs_rf,\n",
    "                feature_names=feature_names,\n",
    "                ax=axes[row, col],\n",
    "                show=False,\n",
    "            )\n",
    "\n",
    "# Adjust subplot spacing\n",
    "plt.subplots_adjust(wspace=0.21, hspace=0.12)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"srs_rf_shap_interaction_values_clay_dependence.png\", dpi=800)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccb361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the 'Clay' feature\n",
    "clay_idx = 1\n",
    "\n",
    "# Create a 3x3 subplot layout, one subplot for each feature pair\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
    "\n",
    "# Plot SHAP interaction dependence plots for each feature with 'Clay'\n",
    "for i, feature_name in enumerate(features):\n",
    "    if i >= 2:\n",
    "        i = i - 1\n",
    "        if feature_name != \"Clay\":\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            shap.dependence_plot(\n",
    "                (clay_idx, i + 1),\n",
    "                shap_values_fscs_rf_inter,\n",
    "                X_train_fscs_rf,\n",
    "                feature_names=feature_names,\n",
    "                ax=axes[row, col],\n",
    "                show=False,\n",
    "            )\n",
    "    else:\n",
    "        if feature_name != \"Clay\":\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            shap.dependence_plot(\n",
    "                (clay_idx, i),\n",
    "                shap_values_fscs_rf_inter,\n",
    "                X_train_fscs_rf,\n",
    "                feature_names=feature_names,\n",
    "                ax=axes[row, col],\n",
    "                show=False,\n",
    "            )\n",
    "\n",
    "# Adjust subplot spacing\n",
    "plt.subplots_adjust(wspace=0.21, hspace=0.12)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"fscs_rf_shap_interaction_values_clay_dependence.png\", dpi=800)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the 'Clay' feature\n",
    "clay_idx = 1\n",
    "\n",
    "# Create a 3x3 subplot layout, one subplot for each feature pair\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
    "\n",
    "# Plot SHAP interaction dependence plots for each feature with 'Clay'\n",
    "for i, feature_name in enumerate(features):\n",
    "    if i >= 2:\n",
    "        i = i - 1\n",
    "        if feature_name != \"Clay\":\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            shap.dependence_plot(\n",
    "                (clay_idx, i + 1),\n",
    "                shap_values_srs_lgb_inter,\n",
    "                X_train_srs_lgb,\n",
    "                feature_names=feature_names,\n",
    "                ax=axes[row, col],\n",
    "                show=False,\n",
    "            )\n",
    "    else:\n",
    "        if feature_name != \"Clay\":\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            shap.dependence_plot(\n",
    "                (clay_idx, i),\n",
    "                shap_values_srs_lgb_inter,\n",
    "                X_train_srs_lgb,\n",
    "                feature_names=feature_names,\n",
    "                ax=axes[row, col],\n",
    "                show=False,\n",
    "            )\n",
    "\n",
    "# Adjust subplot spacing\n",
    "plt.subplots_adjust(wspace=0.21, hspace=0.12)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"srs_lgb_shap_interaction_values_clay_dependence.png\", dpi=800)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the 'Clay' feature\n",
    "clay_idx = 1\n",
    "\n",
    "# Create a 3x3 subplot layout, one subplot for each feature pair\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
    "\n",
    "# Plot SHAP interaction dependence plots for each feature with 'Clay'\n",
    "for i, feature_name in enumerate(features):\n",
    "    if i >= 2:\n",
    "        i = i - 1\n",
    "        if feature_name != \"Clay\":\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            shap.dependence_plot(\n",
    "                (clay_idx, i + 1),\n",
    "                shap_values_fscs_lgb_inter,\n",
    "                X_train_fscs_lgb,\n",
    "                feature_names=feature_names,\n",
    "                ax=axes[row, col],\n",
    "                show=False,\n",
    "            )\n",
    "    else:\n",
    "        if feature_name != \"Clay\":\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            shap.dependence_plot(\n",
    "                (clay_idx, i),\n",
    "                shap_values_fscs_lgb_inter,\n",
    "                X_train_fscs_lgb,\n",
    "                feature_names=feature_names,\n",
    "                ax=axes[row, col],\n",
    "                show=False,\n",
    "            )\n",
    "\n",
    "# Adjust subplot spacing\n",
    "plt.subplots_adjust(wspace=0.21, hspace=0.12)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"fscs_lgb_shap_interaction_values_clay_dependence.png\", dpi=800)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6d11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define covariates and target variable\n",
    "features = [\n",
    "    \"Db\",\n",
    "    \"Clay\",\n",
    "    \"VFS\",\n",
    "    \"MS\",\n",
    "    \"OC\",\n",
    "    \"Silt\",\n",
    "    \"COS\",\n",
    "    \"FS\",\n",
    "    \"DT\",\n",
    "    \"VCOS\",\n",
    "]\n",
    "\n",
    "# Manually specify x-axis labels and units\n",
    "feature_names = [\n",
    "    \"Db (g/cm$^3$)\",\n",
    "    \"Clay (%)\",\n",
    "    \"VFS (%)\",\n",
    "    \"MS (%)\",\n",
    "    \"OC (%)\",\n",
    "    \"Silt (%)\",\n",
    "    \"COS (%)\",\n",
    "    \"FS (%)\",\n",
    "    \"DT (cm)\",\n",
    "    \"VCOS (%)\",\n",
    "]\n",
    "# Get the index of the 'Clay' feature\n",
    "\n",
    "clay_idx = 1\n",
    "\n",
    "# Create a 3x3 subplot layout, one subplot for each feature pair\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
    "\n",
    "# Plot SHAP interaction dependence plots for each feature with 'Clay'\n",
    "\n",
    "for i, feature_name in enumerate(features):\n",
    "\n",
    "    if i >= 2:\n",
    "\n",
    "        i = i - 1\n",
    "\n",
    "        if feature_name != \"Clay\":\n",
    "            axes[row, col].grid(\n",
    "                True, color=\"gray\", linewidth=0.5, alpha=0.7\n",
    "            )  # Enable grid lines\n",
    "\n",
    "            row = i // 3\n",
    "\n",
    "            col = i % 3\n",
    "\n",
    "            shap.dependence_plot(\n",
    "                (clay_idx, i + 1),\n",
    "                shap_values_fscs_lgb_inter,\n",
    "                X_train_fscs_lgb,\n",
    "                feature_names=feature_names,\n",
    "                ax=axes[row, col],\n",
    "                show=False,\n",
    "            )\n",
    "            axes[row, col].set_ylabel(\n",
    "                f\"SHAP Interaction Value for\\nClay(%) and {feature_names[i]}\"\n",
    "            )\n",
    "\n",
    "    else:\n",
    "\n",
    "        if feature_name != \"Clay\":\n",
    "            axes[row, col].grid(\n",
    "                True, color=\"gray\", linewidth=0.5, alpha=0.7\n",
    "            )  # Enable grid lines\n",
    "\n",
    "            row = i // 3\n",
    "\n",
    "            col = i % 3\n",
    "\n",
    "            shap.dependence_plot(\n",
    "                (clay_idx, i),\n",
    "                shap_values_fscs_lgb_inter,\n",
    "                X_train_fscs_lgb,\n",
    "                feature_names=feature_names,\n",
    "                ax=axes[row, col],\n",
    "                show=False,\n",
    "            )\n",
    "            axes[row, col].set_ylabel(\n",
    "                f\"SHAP Interaction Value for\\nClay(%) and {feature_names[i]}\"\n",
    "            )\n",
    "\n",
    "# Adjust subplot spacing\n",
    "\n",
    "plt.subplots_adjust(wspace=0.21, hspace=0.12)\n",
    "\n",
    "# Save the figure\n",
    "\n",
    "# plt.savefig(\"fscs_lgb_shap_interaction_values_clay_dependence.png\", dpi=800) save manually\n",
    "\n",
    "# Show the plot\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
