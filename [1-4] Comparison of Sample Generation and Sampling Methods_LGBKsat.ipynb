{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b50ed9",
   "metadata": {},
   "source": [
    "This code has been modified from the absolute path in the original version to a relative path, so there may be path bugs. Please be aware of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c2e17",
   "metadata": {},
   "source": [
    "Please note that the cleaned data and sampling data generation part of 1-3 and 1-4 only requires one execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = \"sampleddata/USKSAT_OpenRefined.csv\"\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Keep specific features\n",
    "\n",
    "selected_features = [\n",
    "    \"Ksat_cmhr\",\n",
    "    \"Db\",\n",
    "    \"Clay\",\n",
    "    \"VFS\",\n",
    "    \"MS\",\n",
    "    \"OC\",\n",
    "    \"Silt\",\n",
    "    \"COS\",\n",
    "    \"FS\",\n",
    "    \"Depth.cm_Top\",\n",
    "    \"VCOS\",\n",
    "]\n",
    "\n",
    "df = df[selected_features]\n",
    "\n",
    "\n",
    "# Remove samples with NaN values\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "# Generate a new file path with the suffix \"cleaned\"\n",
    "\n",
    "file_dir, file_name = os.path.split(file_path)\n",
    "\n",
    "file_name_wo_ext, file_ext = os.path.splitext(file_name)\n",
    "\n",
    "new_file_name = f\"{file_name_wo_ext}_cleaned{file_ext}\"\n",
    "\n",
    "new_file_path = os.path.join(file_dir, new_file_name)\n",
    "\n",
    "\n",
    "# Save the processed dataset to the new file\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.to_csv(new_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d1f7e",
   "metadata": {},
   "source": [
    "Please note that the cleaned data and sampling data generation part of 1-3 and 1-4 only requires one execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b41020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = \"sampleddata/USKSAT_OpenRefined.csv\"\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Keep specific features\n",
    "\n",
    "selected_features = [\n",
    "    \"Ksat_cmhr\",\n",
    "    \"Db\",\n",
    "    \"Clay\",\n",
    "    \"VFS\",\n",
    "    \"MS\",\n",
    "    \"OC\",\n",
    "    \"Silt\",\n",
    "    \"COS\",\n",
    "    \"FS\",\n",
    "    \"Depth.cm_Top\",\n",
    "    \"VCOS\",\n",
    "]\n",
    "\n",
    "df = df[selected_features]\n",
    "\n",
    "\n",
    "# Remove samples with NaN values\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "# Generate a new file path with the suffix \"cleaned\"\n",
    "\n",
    "file_dir, file_name = os.path.split(file_path)\n",
    "\n",
    "file_name_wo_ext, file_ext = os.path.splitext(file_name)\n",
    "\n",
    "new_file_name = f\"{file_name_wo_ext}_cleaned{file_ext}\"\n",
    "\n",
    "new_file_path = os.path.join(file_dir, new_file_name)\n",
    "\n",
    "\n",
    "# Save the processed dataset to the new file\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.to_csv(new_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1776554",
   "metadata": {},
   "source": [
    "From the cell below, it can be run independently. This is the sampling section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318af41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import skfuzzy as fuzz\n",
    "import os\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22140d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "file_path = \"sampleddata/USKSAT_OpenRefined_cleaned.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8216146",
   "metadata": {},
   "source": [
    "Fuzzy c-means clustering sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425324be",
   "metadata": {},
   "source": [
    "Type 2: Number of clusters equals the number of sampling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import skfuzzy as fuzz\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Define sample sizes and the number of sets to generate\n",
    "sample_sizes = [1000, 5000, 10000]\n",
    "num_samples_per_size = 20\n",
    "\n",
    "# Remove Ksat_cmhr from df\n",
    "df_nonKsat = df.drop(columns=[\"Ksat_cmhr\"])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_nonKsat)\n",
    "\n",
    "\n",
    "# Define the function to generate sample sets\n",
    "def generate_samples(sample_size, random_state):\n",
    "    n_clusters = sample_size  # Set the number of clusters equal to the sample size\n",
    "\n",
    "    # Perform fuzzy c-means clustering\n",
    "    _, u, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
    "        df_scaled.T,\n",
    "        n_clusters,\n",
    "        2,\n",
    "        error=0.005,\n",
    "        maxiter=1000,\n",
    "        init=None,\n",
    "        seed=random_state,\n",
    "    )\n",
    "\n",
    "    # Initialize variables\n",
    "    sampled_indices = set()\n",
    "    sampled_cluster = np.full(\n",
    "        df_scaled.shape[0], -1\n",
    "    )  # -1 means not sampled, others indicate the corresponding cluster\n",
    "    sample_status = np.full(\n",
    "        (df_scaled.shape[0], n_clusters), 0\n",
    "    )  # 0: not sampled, 1: sampled, 2: reclassified to another cluster after being sampled\n",
    "    remaining_samples_per_cluster = np.ones(\n",
    "        n_clusters, dtype=int\n",
    "    )  # Only 1 sample is taken from each cluster\n",
    "\n",
    "    def resample_cluster(\n",
    "        cluster_idx,\n",
    "        sampled_indices,\n",
    "        sampled_cluster,\n",
    "        sample_status,\n",
    "        remaining_samples_per_cluster,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Resample samples in the specified cluster.\n",
    "\n",
    "        Parameters:\n",
    "        - cluster_idx: index of the current cluster\n",
    "        - sampled_indices: set of sampled indices\n",
    "        - sampled_cluster: cluster assignment for each sample\n",
    "        - sample_status: status of each sample in each cluster\n",
    "        - remaining_samples_per_cluster: remaining sample count for each cluster\n",
    "        \"\"\"\n",
    "        # Select unsampled points\n",
    "        cluster_indices_j = np.where(sample_status[:, cluster_idx] == 0)[0]\n",
    "        # Sort by membership\n",
    "        memberships_j = u[cluster_idx, cluster_indices_j]\n",
    "        sorted_indices_j = cluster_indices_j[np.argsort(-memberships_j)]\n",
    "        # Sample\n",
    "        for idx_j in sorted_indices_j:\n",
    "            if remaining_samples_per_cluster[cluster_idx] <= 0:\n",
    "                break\n",
    "            if sample_status[idx_j, cluster_idx] == 0:\n",
    "                if sampled_cluster[idx_j] == -1:\n",
    "                    # If the sample has not been sampled, sample it directly\n",
    "                    sampled_indices.add(idx_j)\n",
    "                    sampled_cluster[idx_j] = cluster_idx\n",
    "                    sample_status[idx_j, cluster_idx] = 1  # Sampled\n",
    "                    remaining_samples_per_cluster[cluster_idx] -= 1\n",
    "                    return\n",
    "                else:\n",
    "                    # If the sample has already been sampled by another cluster, handle membership conflict\n",
    "                    original_cluster = sampled_cluster[idx_j]\n",
    "                    if u[cluster_idx, idx_j] > u[original_cluster, idx_j]:\n",
    "                        sampled_cluster[idx_j] = cluster_idx\n",
    "                        sample_status[idx_j, original_cluster] = (\n",
    "                            2  # Reclassified to another cluster after being sampled\n",
    "                        )\n",
    "                        sample_status[idx_j, cluster_idx] = 1  # Sampled\n",
    "                        remaining_samples_per_cluster[cluster_idx] -= 1\n",
    "                        remaining_samples_per_cluster[original_cluster] += 1\n",
    "                        # Resample the cluster from which the sample was removed\n",
    "                        resample_cluster(\n",
    "                            original_cluster,\n",
    "                            sampled_indices,\n",
    "                            sampled_cluster,\n",
    "                            sample_status,\n",
    "                            remaining_samples_per_cluster,\n",
    "                        )\n",
    "                        return\n",
    "\n",
    "    # Sample based on membership and handle membership conflicts\n",
    "    for i in range(n_clusters):\n",
    "        cluster_indices = np.arange(df_scaled.shape[0])  # Select all points\n",
    "        if len(cluster_indices) > 0:\n",
    "            # Sort by membership\n",
    "            memberships = u[i, cluster_indices]\n",
    "            sorted_indices = cluster_indices[np.argsort(-memberships)]\n",
    "            # Sample\n",
    "            for idx in sorted_indices:\n",
    "                if remaining_samples_per_cluster[i] <= 0:\n",
    "                    break\n",
    "                if sampled_cluster[idx] == -1:\n",
    "                    sampled_indices.add(idx)\n",
    "                    sampled_cluster[idx] = i\n",
    "                    sample_status[idx, i] = 1  # Sampled\n",
    "                    remaining_samples_per_cluster[i] -= 1\n",
    "                else:\n",
    "                    # If the sample has already been sampled by another cluster, handle membership conflict\n",
    "                    current_cluster = sampled_cluster[idx]\n",
    "                    if u[i, idx] > u[current_cluster, idx]:\n",
    "                        sampled_cluster[idx] = i\n",
    "                        sample_status[idx, current_cluster] = (\n",
    "                            2  # Reclassified to another cluster after being sampled\n",
    "                        )\n",
    "                        sample_status[idx, i] = 1  # Sampled\n",
    "                        remaining_samples_per_cluster[i] -= 1\n",
    "                        remaining_samples_per_cluster[current_cluster] += 1\n",
    "                        # Resample the cluster from which the sample was removed\n",
    "                        resample_cluster(\n",
    "                            current_cluster,\n",
    "                            sampled_indices,\n",
    "                            sampled_cluster,\n",
    "                            sample_status,\n",
    "                            remaining_samples_per_cluster,\n",
    "                        )\n",
    "\n",
    "    # Get the sampled data\n",
    "    sampled_df = df.loc[list(sampled_indices)].copy()\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "# Specify the save path\n",
    "save_path = \"sampleddata/combined_samples_Ksat/\"\n",
    "\n",
    "# Create the save directory if it does not exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define the parallel processing function\n",
    "def process_sample(sample_size, i):\n",
    "    random_state = 42 + i  # Generate random seed\n",
    "    sampled_df = generate_samples(sample_size, random_state)\n",
    "    sampled_df.to_csv(\n",
    "        os.path.join(save_path, f\"FCMtp2sampled_data_{sample_size}_set_{i+1}.csv\"),\n",
    "        index=True,\n",
    "    )\n",
    "    print(\n",
    "        f\"Generated sample set {i+1} for sample size {sample_size} with random state {random_state}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Use joblib for parallel processing\n",
    "Parallel(n_jobs=6, verbose=50)(\n",
    "    delayed(process_sample)(sample_size, i)\n",
    "    for sample_size in sample_sizes\n",
    "    for i in range(num_samples_per_size)\n",
    ")\n",
    "\n",
    "print(\"All sample sets generated and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d82607",
   "metadata": {},
   "source": [
    "CLHS Conditioned Latin hypercube sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clhs import clhs\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_nonKsat)\n",
    "\n",
    "# Define sample sizes and the number of sets to generate\n",
    "sample_sizes = [1000, 5000, 10000]\n",
    "num_samples_per_size = 20\n",
    "\n",
    "# Specify the save path\n",
    "save_path = \"sampleddata/combined_samples_Ksat/\"\n",
    "\n",
    "# Create the save directory if it does not exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define the parallel processing function\n",
    "def parallel_clhs(sample_size, i, seed):\n",
    "    np.random.seed(seed)\n",
    "    clhs_sampled_results = clhs(df_scaled, sample_size)\n",
    "    clhs_sampled_df = df.iloc[clhs_sampled_results[\"sample_indices\"]]\n",
    "    file_path = os.path.join(\n",
    "        save_path, f\"clhs_sampled_data_{sample_size}_set_{i+1}.csv\"\n",
    "    )\n",
    "    clhs_sampled_df.to_csv(file_path)\n",
    "    print(f\"Generated sample set {i+1} for sample size {sample_size}\")\n",
    "\n",
    "\n",
    "# Use joblib for parallel sampling\n",
    "Parallel(n_jobs=5, verbose=30)(\n",
    "    delayed(parallel_clhs)(size, i, np.random.randint(0, 10000))\n",
    "    for size in sample_sizes\n",
    "    for i in range(num_samples_per_size)\n",
    ")\n",
    "\n",
    "print(\"All sample sets generated and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b2ce0",
   "metadata": {},
   "source": [
    "From the cell below, it can be run independently. This is the model construction section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Read the original dataset\n",
    "class_df = pd.read_csv(\n",
    "    \"sampleddata/USKSAT_OpenRefined_cleaned.csv\",\n",
    "    index_col=0,  # Use the first column as the index\n",
    ")\n",
    "\n",
    "# Convert the target variable to natural logarithm\n",
    "class_df[\"Ksat_cmhr\"] = np.log(class_df[\"Ksat_cmhr\"])\n",
    "\n",
    "# Define sample levels and sampling methods\n",
    "sample_levels = [1000, 5000, 10000]\n",
    "\n",
    "sampling_methods = [\n",
    "    \"BalancedSampling_sampled_data\",\n",
    "    \"clhs_sampled_data\",\n",
    "    \"FCMtp2sampled_data\",\n",
    "    \"FSCS_sampled_data\",\n",
    "]\n",
    "\n",
    "# Define covariates and target variable\n",
    "covariates = [\n",
    "    \"Db\",\n",
    "    \"Clay\",\n",
    "    \"VFS\",\n",
    "    \"MS\",\n",
    "    \"OC\",\n",
    "    \"Silt\",\n",
    "    \"COS\",\n",
    "    \"FS\",\n",
    "    \"Depth.cm_Top\",\n",
    "    \"VCOS\",\n",
    "]\n",
    "covariates_FSCS = [\n",
    "    \"Db\",\n",
    "    \"Clay\",\n",
    "    \"VFS\",\n",
    "    \"MS\",\n",
    "    \"OC\",\n",
    "    \"Silt\",\n",
    "    \"COS\",\n",
    "    \"FS\",\n",
    "    \"Depth_cm_Top\",\n",
    "    \"VCOS\",\n",
    "]\n",
    "target = \"Ksat_cmhr\"\n",
    "\n",
    "# Store training and testing sets\n",
    "train_test_data = {}\n",
    "\n",
    "# Loop through each sample level and sampling method\n",
    "for level in sample_levels:\n",
    "    for t in range(1, 21):\n",
    "        for method in sampling_methods:\n",
    "            sample_file = (\n",
    "                f\"sampleddata/combined_samples_Ksat/{method}_{level}_set_{t}.csv\"\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(sample_file):\n",
    "                print(f\"File {sample_file} does not exist. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            sample_data = pd.read_csv(\n",
    "                sample_file, index_col=0\n",
    "            )  # Use the first column as the index\n",
    "            sample_data[target] = np.log(\n",
    "                sample_data[target]\n",
    "            )  # Convert the target variable to natural logarithm\n",
    "            if method == \"FSCS_sampled_data\":\n",
    "                X_train = sample_data[covariates_FSCS]\n",
    "                X_train = X_train.rename(columns={\"Depth_cm_Top\": \"DT\"})\n",
    "            else:\n",
    "                X_train = sample_data[covariates]\n",
    "                X_train = X_train.rename(columns={\"Depth.cm_Top\": \"DT\"})\n",
    "\n",
    "            y_train = sample_data[target]\n",
    "            y_train.name = \"Ks\"\n",
    "            train_indices = sample_data.index\n",
    "\n",
    "            # Generate the corresponding test set\n",
    "            test_data = class_df.drop(train_indices)\n",
    "            X_test = test_data[covariates]\n",
    "            X_test = X_test.rename(columns={\"Depth.cm_Top\": \"DT\"})\n",
    "            y_test = test_data[target]\n",
    "            y_test.name = \"Ks\"\n",
    "\n",
    "            # Store training and testing sets\n",
    "            train_test_data[f\"{method}_{level}_set_{t}\"] = (\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_test,\n",
    "                y_test,\n",
    "            )\n",
    "\n",
    "# Generate SRS sample subsets\n",
    "for level in sample_levels:\n",
    "    for t in range(1, 21):\n",
    "        # Simple random sampling\n",
    "        srs_sample = resample(class_df, n_samples=level, random_state=42 + t)\n",
    "        X_train = srs_sample[covariates]\n",
    "        y_train = srs_sample[target]\n",
    "        train_indices = srs_sample.index\n",
    "\n",
    "        # Generate the corresponding test set\n",
    "        test_data = class_df.drop(train_indices)\n",
    "        X_test = test_data[covariates]\n",
    "        y_test = test_data[target]\n",
    "\n",
    "        X_train = X_train.rename(columns={\"Depth.cm_Top\": \"DT\"})\n",
    "        X_test = X_test.rename(columns={\"Depth.cm_Top\": \"DT\"})\n",
    "        y_train.name = \"Ks\"\n",
    "        y_test.name = \"Ks\"\n",
    "\n",
    "        # Store training and testing sets\n",
    "        train_test_data[f\"SRS_sampled_df_{level}_set_{t}\"] = (\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "        )\n",
    "\n",
    "print(\"All training and testing sets generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8375713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear unnecessary variables to free up memory.\n",
    "del (\n",
    "    class_df,\n",
    "    sample_data,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    train_indices,\n",
    "    test_data,\n",
    "    srs_sample,\n",
    ")\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191257ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import r2_score\n",
    "import gc\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna hyperparameter optimization\n",
    "def objective(trial, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "\n",
    "    Args:\n",
    "    trial: Optuna Trial object for recording trial results.\n",
    "    X_train: Training feature data.\n",
    "    y_train: Training label data.\n",
    "\n",
    "    Returns:\n",
    "    The mean root mean squared error from cross-validation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the LightGBM parameter search space\n",
    "    params = {\n",
    "        \"objective\": \"regression\",  # Regression task\n",
    "        \"metric\": \"rmse\",  # Root mean squared error\n",
    "        \"boosting_type\": \"gbdt\",  # Gradient boosting decision tree\n",
    "        \"seed\": 42,  # Random seed\n",
    "        \"verbosity\": -1,  # Set to -1 to reduce output\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 512),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.1, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.1, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 0, 15),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 100),\n",
    "    }\n",
    "\n",
    "    # Create LightGBM dataset\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    # Perform cross-validation with LightGBM\n",
    "    cv_results = lgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        nfold=10,\n",
    "        stratified=False,\n",
    "        seed=42,\n",
    "        num_boost_round=100,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(\n",
    "                stopping_rounds=10,\n",
    "                verbose=False,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    mean_rmse = cv_results[\"valid rmse-mean\"][-1]\n",
    "    # Store the best num_boost_round in user_attrs\n",
    "    trial.set_user_attr(\"num_boost_round\", len(cv_results[\"valid rmse-mean\"]))\n",
    "\n",
    "    return mean_rmse\n",
    "\n",
    "\n",
    "# Set Optuna log level to WARNING to reduce output\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "\n",
    "# Define the processing function\n",
    "def optimize_and_evaluate(key):\n",
    "    # Create an Optuna study object\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    # Select a dataset for optimization and evaluation\n",
    "    X_train, y_train, X_test, y_test = train_test_data[key]\n",
    "    print(f\"Processing dataset: {key}\")\n",
    "\n",
    "    # Run hyperparameter optimization, set to a maximum of 100 trials\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train, y_train),\n",
    "        n_trials=100,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Add necessary parameters\n",
    "    best_params.update(\n",
    "        {\n",
    "            \"objective\": \"regression\",\n",
    "            \"metric\": \"rmse\",\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    return {\n",
    "        \"dataset\": key,\n",
    "        \"best_params\": best_params,\n",
    "        \"num_boost_round\": study.best_trial.user_attrs.get(\"num_boost_round\", 1000),\n",
    "    }\n",
    "\n",
    "\n",
    "# Process each dataset one by one\n",
    "results = []\n",
    "for key in train_test_data.keys():\n",
    "    result = optimize_and_evaluate(key)\n",
    "    results.append(result)\n",
    "\n",
    "# Convert results to DataFrame and print\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"lgbKsatresults.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df9506e",
   "metadata": {},
   "source": [
    "From the cell below, it can be run independently. This is the compare analysis section. Including SHAP and Model Performance Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Read the original dataset\n",
    "class_df = pd.read_csv(\n",
    "    \"sampleddata/USKSAT_OpenRefined_cleaned.csv\",\n",
    "    index_col=0,  # Use the first column as index\n",
    ")\n",
    "\n",
    "# Convert the target variable to natural logarithm\n",
    "class_df[\"Ksat_cmhr\"] = np.log(class_df[\"Ksat_cmhr\"])\n",
    "\n",
    "# Define sample sizes and sampling methods\n",
    "sample_levels = [1000, 5000, 10000]\n",
    "\n",
    "sampling_methods = [\n",
    "    \"BalancedSampling_sampled_data\",\n",
    "    \"clhs_sampled_data\",\n",
    "    \"FCMtp2sampled_data\",\n",
    "    \"FSCS_sampled_data\",\n",
    "]\n",
    "\n",
    "# Define covariates and target variable\n",
    "covariates = [\n",
    "    \"Db\",\n",
    "    \"Clay\",\n",
    "    \"VFS\",\n",
    "    \"MS\",\n",
    "    \"OC\",\n",
    "    \"Silt\",\n",
    "    \"COS\",\n",
    "    \"FS\",\n",
    "    \"Depth.cm_Top\",\n",
    "    \"VCOS\",\n",
    "]\n",
    "covariates_FSCS = [\n",
    "    \"Db\",\n",
    "    \"Clay\",\n",
    "    \"VFS\",\n",
    "    \"MS\",\n",
    "    \"OC\",\n",
    "    \"Silt\",\n",
    "    \"COS\",\n",
    "    \"FS\",\n",
    "    \"Depth_cm_Top\",\n",
    "    \"VCOS\",\n",
    "]\n",
    "target = \"Ksat_cmhr\"\n",
    "\n",
    "# Store training and testing sets\n",
    "train_test_data = {}\n",
    "\n",
    "# Loop through each sample size and sampling method\n",
    "for level in sample_levels:\n",
    "    for t in range(1, 21):\n",
    "        for method in sampling_methods:\n",
    "            sample_file = (\n",
    "                f\"sampleddata/combined_samples_Ksat/{method}_{level}_set_{t}.csv\"\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(sample_file):\n",
    "                print(f\"File {sample_file} does not exist. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            sample_data = pd.read_csv(\n",
    "                sample_file, index_col=0\n",
    "            )  # Use the first column as index\n",
    "            sample_data[target] = np.log(\n",
    "                sample_data[target]\n",
    "            )  # Convert the target variable to natural logarithm\n",
    "            if method == \"FSCS_sampled_data\":\n",
    "                X_train = sample_data[covariates_FSCS]\n",
    "                X_train = X_train.rename(columns={\"Depth_cm_Top\": \"DT\"})\n",
    "            else:\n",
    "                X_train = sample_data[covariates]\n",
    "                X_train = X_train.rename(columns={\"Depth.cm_Top\": \"DT\"})\n",
    "\n",
    "            y_train = sample_data[target]\n",
    "            y_train.name = \"Ks\"\n",
    "            train_indices = sample_data.index\n",
    "\n",
    "            # Generate the corresponding test set\n",
    "            test_data = class_df.drop(train_indices)\n",
    "            X_test = test_data[covariates]\n",
    "            X_test = X_test.rename(columns={\"Depth.cm_Top\": \"DT\"})\n",
    "            y_test = test_data[target]\n",
    "            y_test.name = \"Ks\"\n",
    "\n",
    "            # Store training and testing sets\n",
    "            train_test_data[f\"{method}_{level}_set_{t}\"] = (\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_test,\n",
    "                y_test,\n",
    "            )\n",
    "\n",
    "# Generate SRS sample subsets\n",
    "for level in sample_levels:\n",
    "    for t in range(1, 21):\n",
    "        # Simple random sampling\n",
    "        srs_sample = resample(class_df, n_samples=level, random_state=42 + t)\n",
    "        X_train = srs_sample[covariates]\n",
    "        y_train = srs_sample[target]\n",
    "        train_indices = srs_sample.index\n",
    "\n",
    "        # Generate the corresponding test set\n",
    "        test_data = class_df.drop(train_indices)\n",
    "        X_test = test_data[covariates]\n",
    "        y_test = test_data[target]\n",
    "\n",
    "        X_train = X_train.rename(columns={\"Depth.cm_Top\": \"DT\"})\n",
    "        X_test = X_test.rename(columns={\"Depth.cm_Top\": \"DT\"})\n",
    "        y_train.name = \"Ks\"\n",
    "        y_test.name = \"Ks\"\n",
    "\n",
    "        # Store training and testing sets\n",
    "        train_test_data[f\"SRS_sampled_df_{level}_set_{t}\"] = (\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "        )\n",
    "\n",
    "print(\"All training and testing sets generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953aa55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unnecessary variables to free memory\n",
    "del (\n",
    "    class_df,\n",
    "    sample_data,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    train_indices,\n",
    "    test_data,\n",
    "    srs_sample,\n",
    ")\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a74502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the saved DataFrame file\n",
    "results_df = pd.read_csv(\"rfKsatresults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de51f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import fasttreeshap\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# Create a dictionary to store models\n",
    "models = {}\n",
    "\n",
    "# Specify the save path\n",
    "save_path = \"codepart/LGBKsatpkl\"\n",
    "# Ensure the save path exists\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def is_file_valid(filename):\n",
    "    try:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            joblib.load(f)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"File {filename} is invalid: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_dataset(index, row):\n",
    "    dataset = row[\"dataset\"]\n",
    "\n",
    "    best_params = row[\"best_params\"]\n",
    "    # best_params = eval(best_params)  # Convert string to dictionary if needed\n",
    "\n",
    "    nboost_round = row[\"num_boost_round\"]\n",
    "\n",
    "    # Get the corresponding dataset\n",
    "    X_train, y_train, X_test, y_test = train_test_data[dataset]\n",
    "\n",
    "    # File paths\n",
    "    model_filename = os.path.join(save_path, f\"{dataset}_model.pkl\")\n",
    "    shap_values_filename_v2 = os.path.join(save_path, f\"{dataset}_shap_values_v2.pkl\")\n",
    "    shap_interaction_values_filename_v1 = os.path.join(\n",
    "        save_path, f\"{dataset}_shap_interaction_values_v1.pkl\"\n",
    "    )\n",
    "    shap_explainer_v2_filename = os.path.join(\n",
    "        save_path, f\"{dataset}_shap_explainer_v2.pkl\"\n",
    "    )\n",
    "    shap_explainer_v1_filename = os.path.join(\n",
    "        save_path, f\"{dataset}_shap_explainer_v1.pkl\"\n",
    "    )\n",
    "\n",
    "    # Check if all files already exist and are valid\n",
    "    if (\n",
    "        os.path.exists(model_filename)\n",
    "        and is_file_valid(model_filename)\n",
    "        and os.path.exists(shap_values_filename_v2)\n",
    "        and is_file_valid(shap_values_filename_v2)\n",
    "        and os.path.exists(shap_interaction_values_filename_v1)\n",
    "        and is_file_valid(shap_interaction_values_filename_v1)\n",
    "    ):\n",
    "        print(f\"{dataset} has already been processed, loading files directly.\")\n",
    "        rg = joblib.load(model_filename)\n",
    "        shap_values_v2 = joblib.load(shap_values_filename_v2)\n",
    "        shap_interaction_values_v1 = joblib.load(shap_interaction_values_filename_v1)\n",
    "\n",
    "    else:\n",
    "        if os.path.exists(model_filename) and is_file_valid(model_filename):\n",
    "            rg = joblib.load(model_filename)\n",
    "        else:\n",
    "            # Train the model with the best hyperparameters\n",
    "            dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "            dtest = lgb.Dataset(X_test, label=y_test, reference=dtrain)\n",
    "            rg = lgb.train(\n",
    "                best_params,\n",
    "                dtrain,\n",
    "                valid_sets=[dtest],\n",
    "                num_boost_round=nboost_round,\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(\n",
    "                        stopping_rounds=10,\n",
    "                        verbose=False,\n",
    "                    ),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        # Save the trained model\n",
    "        joblib.dump(rg, model_filename, compress=9)\n",
    "\n",
    "        if os.path.exists(shap_values_filename_v2) and is_file_valid(\n",
    "            shap_values_filename_v2\n",
    "        ):\n",
    "            shap_values_v2 = joblib.load(shap_values_filename_v2)\n",
    "        else:\n",
    "            # Perform SHAP analysis to analyze feature importance and interaction\n",
    "            shap_explainer_v2 = fasttreeshap.TreeExplainer(\n",
    "                rg, algorithm=\"v2\", n_jobs=-1\n",
    "            )\n",
    "            shap_values_v2 = shap_explainer_v2(X_train).values\n",
    "            # Save SHAP values\n",
    "            joblib.dump(shap_values_v2, shap_values_filename_v2, compress=9)\n",
    "            # Save SHAP explainer\n",
    "            joblib.dump(shap_explainer_v2, shap_explainer_v2_filename, compress=9)\n",
    "\n",
    "        if os.path.exists(shap_interaction_values_filename_v1) and is_file_valid(\n",
    "            shap_interaction_values_filename_v1\n",
    "        ):\n",
    "            shap_interaction_values_v1 = joblib.load(\n",
    "                shap_interaction_values_filename_v1\n",
    "            )\n",
    "        else:\n",
    "            shap_explainer_v1 = fasttreeshap.TreeExplainer(\n",
    "                rg, algorithm=\"v1\", n_jobs=-1\n",
    "            )\n",
    "            shap_interaction_values_v1 = shap_explainer_v1(\n",
    "                X_train, interactions=True\n",
    "            ).values\n",
    "            joblib.dump(\n",
    "                shap_interaction_values_v1,\n",
    "                shap_interaction_values_filename_v1,\n",
    "                compress=9,\n",
    "            )\n",
    "            joblib.dump(shap_explainer_v1, shap_explainer_v1_filename, compress=9)\n",
    "\n",
    "    # Final print statement\n",
    "    print(f\"Finished processing {dataset}\")\n",
    "\n",
    "\n",
    "# Use joblib for parallel processing and show progress\n",
    "with Parallel(n_jobs=-1, verbose=50) as parallel:\n",
    "    parallel(\n",
    "        delayed(process_dataset)(index, row) for index, row in results_df.iterrows()\n",
    "    )\n",
    "\n",
    "print(\"All datasets have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_log_error\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Specify the save path\n",
    "save_path = \"codepart/LGBKsatpkl\"\n",
    "# Ensure the save path exists\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def root_mean_squared_log_error(true, pred):\n",
    "    square_error = np.square((np.log(true + 1) - np.log(pred + 1)))\n",
    "    mean_square_log_error = np.mean(square_error)\n",
    "    rmsle_loss = np.sqrt(mean_square_log_error)\n",
    "    return rmsle_loss\n",
    "\n",
    "\n",
    "def calculate_metrics(index, row):\n",
    "    dataset = row[\"dataset\"]\n",
    "\n",
    "    # Get the corresponding dataset\n",
    "    _, _, X_test, y_test = train_test_data[dataset]\n",
    "\n",
    "    # File path\n",
    "    model_filename = os.path.join(save_path, f\"{dataset}_model.pkl\")\n",
    "\n",
    "    # Check if the model file already exists\n",
    "    if os.path.exists(model_filename):\n",
    "        print(f\"{dataset} has already been processed, loading file directly.\")\n",
    "        clf = joblib.load(model_filename)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate R² score\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Calculate RMSLE score\n",
    "        rmsle_loss = root_mean_squared_log_error(y_test, y_pred)\n",
    "\n",
    "        # Return results\n",
    "        return index, r2, rmsle_loss\n",
    "    else:\n",
    "        print(f\"Model file for {dataset} does not exist, skipping.\")\n",
    "        return index, None, None\n",
    "\n",
    "\n",
    "# Use joblib for parallel processing\n",
    "results = Parallel(n_jobs=-1, verbose=50)(\n",
    "    delayed(calculate_metrics)(index, row) for index, row in results_df.iterrows()\n",
    ")\n",
    "\n",
    "# Update results_df\n",
    "for index, r2, rmsle_value in results:\n",
    "    if r2 is not None:\n",
    "        results_df.loc[index, \"r2\"] = r2\n",
    "        results_df.loc[index, \"rmsle\"] = rmsle_value\n",
    "\n",
    "print(\"R² and RMSLE scores for all datasets have been calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"lgbKsatresults_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62916b",
   "metadata": {},
   "source": [
    "From the cell below, it can be run independently. Figure drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15262bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the saved DataFrame file\n",
    "results_df = pd.read_csv(\"lgbKsatresults_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc736cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a new variable to store extracted sampling methods and dataset levels\n",
    "results_with_methods = results_df.copy()\n",
    "results_with_methods[\"sampling_method\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[0]\n",
    ")\n",
    "results_with_methods[\"dataset_level\"] = results_with_methods[\"dataset\"].apply(\n",
    "    lambda x: x.split(\"_\")[-3]\n",
    ")\n",
    "\n",
    "# Set font size\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "# Draw boxplots\n",
    "plt.figure(figsize=(10, 16))  # Adjust figure width\n",
    "\n",
    "# Draw boxplot for R2\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.boxplot(\n",
    "    x=\"sampling_method\",\n",
    "    y=\"r2\",\n",
    "    hue=\"dataset_level\",\n",
    "    data=results_with_methods,\n",
    "    width=0.6,\n",
    ")\n",
    "plt.title(\"R2 Boxplot\", fontsize=20)\n",
    "plt.xlabel(\"Sampling Method\", fontsize=18, labelpad=20)\n",
    "plt.ylabel(\"R2\", fontsize=18, labelpad=20)\n",
    "plt.grid(True, which=\"major\", axis=\"y\", linestyle=\"-\", linewidth=0.6)\n",
    "plt.grid(True, which=\"minor\", axis=\"y\", linestyle=\"--\", linewidth=0.25)\n",
    "plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.05))\n",
    "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.01))\n",
    "plt.legend(title=\"Dataset Level\", loc=\"lower left\")  # Add legend\n",
    "\n",
    "# Draw boxplot for RMSLE\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.boxplot(\n",
    "    x=\"sampling_method\", y=\"rmsle\", hue=\"dataset_level\", data=results_with_methods\n",
    ")\n",
    "plt.title(\"RMSLE Boxplot\", fontsize=20)\n",
    "plt.xlabel(\"Sampling Method\", fontsize=18, labelpad=20)\n",
    "plt.ylabel(\"Test RMSLE\", fontsize=18, labelpad=20)\n",
    "plt.grid(True, which=\"major\", axis=\"y\", linestyle=\"-\", linewidth=0.6)\n",
    "plt.grid(True, which=\"minor\", axis=\"y\", linestyle=\"--\", linewidth=0.25)\n",
    "plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.05))\n",
    "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.01))\n",
    "plt.gca().invert_yaxis()  # Invert y-axis direction\n",
    "plt.legend(title=\"Dataset Level\", loc=\"lower left\")  # Add legend\n",
    "\n",
    "# Specify x-axis labels from left to right\n",
    "sampling_methods = [\n",
    "    \"Balance\\nSampling\",\n",
    "    \"CLHS\",\n",
    "    \"FCM\\nClu = level\",\n",
    "    \"FSCS\",\n",
    "    \"SRS\",\n",
    "]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.xticks(ticks=range(len(sampling_methods)), labels=sampling_methods)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.xticks(ticks=range(len(sampling_methods)), labels=sampling_methods)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"LGBKsatresults_plot.jpg\", format=\"jpg\", dpi=800, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median R² and RMSLE for each sampling method\n",
    "median_metrics = (\n",
    "    results_with_methods.groupby([\"sampling_method\", \"dataset_level\"])[[\"r2\", \"rmsle\"]]\n",
    "    .median()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Print the median results\n",
    "print(median_metrics)\n",
    "median_metrics.to_csv(\"rfKsatmedian_metrics.csv\", index=False)\n",
    "\n",
    "\n",
    "# Calculate the percentage difference of each method relative to SRS of the same level\n",
    "def calculate_percentage_difference(group):\n",
    "    srs_values = group[group[\"sampling_method\"] == \"SRS\"]\n",
    "    if srs_values.empty:\n",
    "        return group.set_index(\"sampling_method\") * float(\"nan\")\n",
    "    srs_values = srs_values.iloc[0][[\"r2\", \"rmsle\"]]\n",
    "    return (group.set_index(\"sampling_method\")[[\"r2\", \"rmsle\"]] / srs_values - 1) * 100\n",
    "\n",
    "\n",
    "# Group by dataset_level and calculate the percentage difference for each method relative to SRS\n",
    "percentage_diff = (\n",
    "    median_metrics.groupby(\"dataset_level\")\n",
    "    .apply(calculate_percentage_difference)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge into a single table\n",
    "combined_metrics = pd.merge(\n",
    "    median_metrics,\n",
    "    percentage_diff,\n",
    "    on=[\"sampling_method\", \"dataset_level\"],\n",
    "    suffixes=(\"\", \"_percentage_diff\"),\n",
    ")\n",
    "\n",
    "# Print the combined table results\n",
    "print(\"Combined table results:\")\n",
    "print(combined_metrics)\n",
    "combined_metrics.to_csv(\"lgbKsatmedian_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
